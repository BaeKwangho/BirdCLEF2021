{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchlibrosa.stft import Spectrogram, LogmelFilterBank\n",
    "from torchlibrosa.augmentation import SpecAugmentation\n",
    "import os\n",
    "import librosa\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import math\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate(x, ratio):\n",
    "    '''Interpolate the prediction to have the same time_steps as the target. \n",
    "    The time_steps mismatch is caused by maxpooling in CNN. \n",
    "    \n",
    "    Args:\n",
    "      x: (batch_size, time_steps, classes_num)\n",
    "      ratio: int, ratio to upsample\n",
    "    '''\n",
    "    (batch_size, time_steps, classes_num) = x.shape\n",
    "    upsampled = x[:, :, None, :].repeat(1, 1, ratio, 1)\n",
    "    upsampled = upsampled.reshape(batch_size, time_steps * ratio, classes_num)\n",
    "    return upsampled\n",
    "\n",
    "def init_layer(layer):\n",
    "    \"\"\"Initialize a Linear or Convolutional layer. \"\"\"\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    " \n",
    "    if hasattr(layer, 'bias'):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.)\n",
    "            \n",
    "    \n",
    "def init_bn(bn):\n",
    "    \"\"\"Initialize a Batchnorm layer. \"\"\"\n",
    "    bn.bias.data.fill_(0.)\n",
    "    bn.weight.data.fill_(1.)\n",
    "\n",
    "\n",
    "def init_gru(rnn):\n",
    "    \"\"\"Initialize a GRU layer. \"\"\"\n",
    "    \n",
    "    def _concat_init(tensor, init_funcs):\n",
    "        (length, fan_out) = tensor.shape\n",
    "        fan_in = length // len(init_funcs)\n",
    "    \n",
    "        for (i, init_func) in enumerate(init_funcs):\n",
    "            init_func(tensor[i * fan_in : (i + 1) * fan_in, :])\n",
    "        \n",
    "    def _inner_uniform(tensor):\n",
    "        fan_in = nn.init._calculate_correct_fan(tensor, 'fan_in')\n",
    "        nn.init.uniform_(tensor, -math.sqrt(3 / fan_in), math.sqrt(3 / fan_in))\n",
    "    \n",
    "    for i in range(rnn.num_layers):\n",
    "        _concat_init(\n",
    "            getattr(rnn, 'weight_ih_l{}'.format(i)),\n",
    "            [_inner_uniform, _inner_uniform, _inner_uniform]\n",
    "        )\n",
    "        torch.nn.init.constant_(getattr(rnn, 'bias_ih_l{}'.format(i)), 0)\n",
    "\n",
    "        _concat_init(\n",
    "            getattr(rnn, 'weight_hh_l{}'.format(i)),\n",
    "            [_inner_uniform, _inner_uniform, nn.init.orthogonal_]\n",
    "        )\n",
    "        torch.nn.init.constant_(getattr(rnn, 'bias_hh_l{}'.format(i)), 0)\n",
    "        \n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \n",
    "        super(ConvBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, \n",
    "                              out_channels=out_channels,\n",
    "                              kernel_size=(3, 3), stride=(1, 1),\n",
    "                              padding=(1, 1), bias=False)\n",
    "                              \n",
    "        self.conv2 = nn.Conv2d(in_channels=out_channels, \n",
    "                              out_channels=out_channels,\n",
    "                              kernel_size=(3, 3), stride=(1, 1),\n",
    "                              padding=(1, 1), bias=False)\n",
    "                              \n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.init_weight()\n",
    "        \n",
    "    def init_weight(self):\n",
    "        init_layer(self.conv1)\n",
    "        init_layer(self.conv2)\n",
    "        init_bn(self.bn1)\n",
    "        init_bn(self.bn2)\n",
    "\n",
    "        \n",
    "    def forward(self, input, pool_size=(2, 2), pool_type='avg'):\n",
    "        \n",
    "        x = input\n",
    "        x = F.relu_(self.bn1(self.conv1(x)))\n",
    "        x = F.relu_(self.bn2(self.conv2(x)))\n",
    "        if pool_type == 'max':\n",
    "            x = F.max_pool2d(x, kernel_size=pool_size)\n",
    "        elif pool_type == 'avg':\n",
    "            x = F.avg_pool2d(x, kernel_size=pool_size)\n",
    "        elif pool_type == 'avg+max':\n",
    "            x1 = F.avg_pool2d(x, kernel_size=pool_size)\n",
    "            x2 = F.max_pool2d(x, kernel_size=pool_size)\n",
    "            x = x1 + x2\n",
    "        else:\n",
    "            raise Exception('Incorrect argument!')\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "class AttBlock(nn.Module):\n",
    "    def __init__(self, n_in, n_out, activation='linear', temperature=1.):\n",
    "        super(AttBlock, self).__init__()\n",
    "        \n",
    "        self.activation = activation\n",
    "        self.temperature = temperature\n",
    "        self.att = nn.Conv1d(in_channels=n_in, out_channels=n_out, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "        self.cla = nn.Conv1d(in_channels=n_in, out_channels=n_out, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "        \n",
    "        self.bn_att = nn.BatchNorm1d(n_out)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        init_layer(self.att)\n",
    "        init_layer(self.cla)\n",
    "        init_bn(self.bn_att)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (n_samples, n_in, n_time)\n",
    "        tmp = self.att(x)\n",
    "        tmp = torch.clamp(tmp, -10, 10)\n",
    "        att = torch.exp(tmp / self.temperature) + 1e-6\n",
    "        norm_att = att / torch.sum(att, dim=2)[:, :, None]\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        return x, norm_att, cla\n",
    "\n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == 'linear':\n",
    "            return x\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return torch.sigmoid(x)\n",
    "    \n",
    "\n",
    "class Tmodel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Tmodel,self).__init__()\n",
    "        \n",
    "        SPEC_HEIGHT = 128\n",
    "        SPEC_WIDTH = 256\n",
    "        NUM_MELS = SPEC_HEIGHT\n",
    "        HOP_LENGTH = int(32000 * 5 / (SPEC_WIDTH - 1)) # sample rate * duration / spec width - 1 == 627\n",
    "        FMIN = 500\n",
    "        FMAX = 12500\n",
    "        classes_num = 398\n",
    "        self.interpolate_ratio = 8\n",
    "        \n",
    "        self.spectrogram_extractor  = Spectrogram(\n",
    "                    n_fft=2048,\n",
    "                    hop_length=HOP_LENGTH,\n",
    "                    freeze_parameters=True)\n",
    "        \n",
    "        self.logmel_extractor = LogmelFilterBank(sr=32000,\n",
    "            n_mels=NUM_MELS, fmin=FMIN, fmax=FMAX, freeze_parameters=True)\n",
    "\n",
    "        self.spec_augmenter = SpecAugmentation(time_drop_width=64, time_stripes_num=2, \n",
    "            freq_drop_width=8, freq_stripes_num=2)\n",
    "        \n",
    "        self.bn0 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.conv_block1 = ConvBlock(in_channels=1, out_channels=64)\n",
    "        self.conv_block2 = ConvBlock(in_channels=64, out_channels=128)\n",
    "        self.conv_block3 = ConvBlock(in_channels=128, out_channels=256)\n",
    "        self.conv_block4 = ConvBlock(in_channels=256, out_channels=512)\n",
    "\n",
    "        self.gru = nn.GRU(input_size=512, hidden_size=256, num_layers=1, \n",
    "            bias=True, batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.att_block = AttBlock(n_in=512, n_out=classes_num, activation='sigmoid')\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        init_bn(self.bn0)\n",
    "        init_gru(self.gru)\n",
    "        \n",
    "    def forward(self,input,mixup_lambda=None):\n",
    "        \n",
    "        x = self.spectrogram_extractor(input)   # (batch_size, 1, time_steps, freq_bins)\n",
    "        x = self.logmel_extractor(x)    # (batch_size, 1, time_steps, mel_bins)\n",
    "        \n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "        \n",
    "        if self.training:\n",
    "            x = self.spec_augmenter(x)\n",
    "\n",
    "        # Mixup on spectrogram\n",
    "        if self.training and mixup_lambda is not None:\n",
    "            x = do_mixup(x, mixup_lambda)\n",
    "        \n",
    "        x = self.conv_block1(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = self.conv_block2(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = self.conv_block3(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = self.conv_block4(x, pool_size=(1, 1), pool_type='avg')\n",
    "\n",
    "        x = torch.mean(x, dim=3)\n",
    "        x = x.transpose(1, 2)   # (batch_size, time_steps, channels)\n",
    "        (x, _) = self.gru(x)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        (clipwise_output, norm_att, cla) = self.att_block(x)\n",
    "        \"\"\"cla: (batch_size, classes_num, time_stpes)\"\"\"\n",
    "        \n",
    "        # Framewise output\n",
    "        framewise_output = cla.transpose(1, 2)\n",
    "        framewise_output = interpolate(framewise_output, self.interpolate_ratio)\n",
    "        \n",
    "        # Clipwise output\n",
    "        clipwise_output = torch.mean(framewise_output, dim=1)\n",
    "        \n",
    "        output_dict = {\n",
    "            'framewise_output': framewise_output, \n",
    "            'clipwise_output': clipwise_output, \n",
    "            'embedding': cla}\n",
    "            \n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepareing\n",
    "import random\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "df = pd.read_csv('./asset/birdclef-2021/train_metadata.csv')\n",
    "\n",
    "folder_path = './asset/birdclef-2021/train_short_audio/'\n",
    "\n",
    "all_list = list()\n",
    "\n",
    "all_sec = list()\n",
    "for pri,sec,file in zip(df['primary_label'],df['secondary_labels'],df['filename']):\n",
    "    sec_2 = list(sec.replace(\"'\",'').replace('[','').replace(']','').replace(' ','').split(','))  \n",
    "    sec_2.append(pri)\n",
    "    if sec_2[0]=='':\n",
    "        sec_2=sec_2[1:]\n",
    "    for bird in sec_2:\n",
    "        all_sec.append(bird)\n",
    "    filename = os.path.join(folder_path,pri,file)\n",
    "    all_list.append({'path':filename,'bird':sec_2})\n",
    "\n",
    "class testData(Dataset):\n",
    "    def __init__(self,all_list):\n",
    "        self.all_list = all_list\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.all_list)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.all_list[idx]\n",
    "    \n",
    "blist = list(set(all_sec))\n",
    "classes_num= len(blist)\n",
    "\n",
    "# collate_fn by class type, handling with parameters/frame\n",
    "'''\n",
    "class Collator(object):\n",
    "    def __init__(self,blist,frame_sec=7,sr=32000,number_of_frame=20,classes_num=397):\n",
    "        self.frame_sec = frame_sec\n",
    "        self.sr = sr\n",
    "        self.number_of_frame = number_of_frame\n",
    "        self.classes_num = classes_num\n",
    "        self.blist = blist\n",
    "        \n",
    "    def __call__(self,batch):\n",
    "        birds = list()\n",
    "        frames = list()\n",
    "        duration = self.frame_sec * self.sr\n",
    "        \n",
    "        batch_ind = 0\n",
    "        wav, _ = librosa.load(batch[batch_ind]['path'],sr=32000)\n",
    "        \n",
    "        wav_ind = 0\n",
    "        while len(frames) < self.number_of_frame:\n",
    "            if wav_ind+duration>len(wav) and batch_ind<len(batch)-1:\n",
    "                batch_ind += 1\n",
    "                wav, _ = librosa.load(batch[batch_ind]['path'],sr=32000)\n",
    "                wav_ind = 0\n",
    "            else:\n",
    "                frame = wav[wav_ind:wav_ind+duration]\n",
    "                frames.append(frame[np.newaxis])\n",
    "                \n",
    "                bird_arr = np.zeros((1,self.classes_num))\n",
    "                for bird in batch[batch_ind]['bird']:\n",
    "                    bird_arr[0][self.blist.index(bird)]=1\n",
    "                    birds.append(bird_arr)\n",
    "                wav_ind += duration\n",
    "                \n",
    "        print(frames)\n",
    "        frames = np.concatenate(frames)\n",
    "        birds = np.concatenate(birds)\n",
    "        \n",
    "        return frames,birds\n",
    "'''\n",
    "class BCEFocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        bce_loss = nn.BCEWithLogitsLoss(reduction='none')(preds, targets)\n",
    "        probas = torch.sigmoid(preds)\n",
    "        loss = targets * self.alpha * \\\n",
    "            (1. - probas)**self.gamma * bce_loss + \\\n",
    "            (1. - targets) * probas**self.gamma * bce_loss\n",
    "        loss = loss.mean()\n",
    "        return loss\n",
    "\n",
    "\n",
    "class BCEFocal2WayLoss(nn.Module):\n",
    "    def __init__(self, weights=[1, 1], class_weights=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.focal = BCEFocalLoss()\n",
    "\n",
    "        self.weights = weights\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        input_ = input[\"logit\"]\n",
    "        target = target.float()\n",
    "\n",
    "        framewise_output = input[\"framewise_logit\"]\n",
    "        clipwise_output_with_max, _ = framewise_output.max(dim=1)\n",
    "\n",
    "        loss = self.focal(input_, target)\n",
    "        aux_loss = self.focal(clipwise_output_with_max, target)\n",
    "\n",
    "        return self.weights[0] * loss + self.weights[1] * aux_loss\n",
    "    \n",
    "class BatchCollator(object):\n",
    "    def __init__(self,blist,frame_sec=7,sr=32000,classes_num=398):\n",
    "        self.frame_sec = frame_sec\n",
    "        self.sr = sr\n",
    "        self.classes_num = classes_num\n",
    "        self.blist = blist\n",
    "        self.duration = frame_sec * sr\n",
    "\n",
    "    def __call__(self,batch):\n",
    "        waves = list()\n",
    "        birds = np.zeros((len(batch),self.classes_num))\n",
    "        for i,meta in enumerate(batch):\n",
    "            wav, _ = librosa.load(meta['path'],sr=self.sr)\n",
    "            if len(wav) < self.duration:\n",
    "                wav = np.concatenate([wav,np.zeros((self.duration-len(wav)))])[np.newaxis,:]\n",
    "            else:\n",
    "                ind = random.randint(0,len(wav)-self.duration)\n",
    "                wav = wav[ind:ind+self.duration][np.newaxis,:]\n",
    "            waves.append(wav)\n",
    "            for bird in meta['bird']:\n",
    "                birds[i][blist.index(bird)] = 1\n",
    "\n",
    "        waves = np.concatenate(waves)\n",
    "\n",
    "        return waves,birds\n",
    "\n",
    "dataset = testData(all_list)\n",
    "collator = BatchCollator(blist)\n",
    "#dataloader = DataLoader(dataset,batch_size=1,shuffle=True,collate_fn=make_batch)\n",
    "dataloader = DataLoader(dataset,batch_size=20,shuffle=True,collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "waves =list()\n",
    "wav_path = './asset/birdclef-2021/train_short_audio/acafly/XC109605.ogg'\n",
    "wav , _ = librosa.load(wav_path)\n",
    "\n",
    "waves.append(wav)\n",
    "wav_path2 = './asset/birdclef-2021/train_short_audio/aldfly/XC134932.ogg'\n",
    "wav2 , _ = librosa.load(wav_path2)\n",
    "\n",
    "waves.append(wav2)\n",
    "maxlen = max([len(i) for i in waves])\n",
    "\n",
    "waves = np.concatenate([np.concatenate([i,np.zeros((maxlen-len(i)))])[np.newaxis,:] for i in waves])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4791893d68>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(waves[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from autoth.core import HyperParamsOptimizer\n",
    "\n",
    "class ScoreCalculatorExample(object):\n",
    "    def __init__(self, batch_size, classes_num):\n",
    "        \"\"\"An example of Score calculator. Used to calculate score (such as F1), \n",
    "        given prediction, target and hyper parameters. \n",
    "        \"\"\"\n",
    "        self.N = batch_size     # Number of samples\n",
    "        self.classes_num = classes_num    # Number of classes\n",
    "        random_state = np.random.RandomState(1234)\n",
    "\n",
    "        # Simulate system prediction. Usually a probability. \n",
    "        self.prediction = random_state.uniform(0, 1, (self.N, self.classes_num))\n",
    "\n",
    "        # Simulate ground truth.\n",
    "        self.target = random_state.randint(0, 2, (self.N, self.classes_num))\n",
    "\n",
    "    def __call__(self, params):\n",
    "        \"\"\"Parameters (such as thresholds) are used calculate score.\n",
    "        Args:\n",
    "          params: list of float\n",
    "        Returns:\n",
    "          score: float\n",
    "        \"\"\"\n",
    "        thresholds = params\n",
    "        output = np.zeros_like(self.prediction)\n",
    "\n",
    "        # Threshold to output\n",
    "        for n in range(self.N):\n",
    "            for k in range(self.classes_num):\n",
    "                if self.prediction[n, k] > thresholds[k]:\n",
    "                    output[n, k] = 1\n",
    "\n",
    "        # Calculate score\n",
    "        score = metrics.f1_score(self.target, output, average='macro')\n",
    "\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing hyper parameters ...\n",
      "learning rate: 0.010, total epochs: 10\n",
      "    Hyper parameters: [0.2902, 0.3097, 0.3067, 0.3089, 0.2931, 0.303, 0.3024, 0.3, 0.3091, 0.2902, 0.3029, 0.3071, 0.3, 0.3051, 0.3065, 0.3091, 0.309, 0.291, 0.3, 0.304, 0.2917, 0.3088, 0.2903, 0.2934, 0.3, 0.3022, 0.3029, 0.2902, 0.2909, 0.2949, 0.2963, 0.303, 0.2906, 0.2902, 0.293, 0.3092, 0.2912, 0.303, 0.2903, 0.3043, 0.3, 0.3061, 0.2953, 0.2902, 0.2945, 0.2944, 0.2917, 0.3076, 0.2902, 0.3091, 0.3, 0.2947, 0.2919, 0.3019, 0.3057, 0.2903, 0.3023, 0.3, 0.2969, 0.2915, 0.2938, 0.3, 0.3056, 0.2926, 0.2915, 0.2944, 0.3065, 0.2939, 0.2925, 0.3, 0.2935, 0.3027, 0.2942, 0.2912, 0.2946, 0.2916, 0.2913, 0.2912, 0.291, 0.3084, 0.3069, 0.2923, 0.3, 0.3, 0.309, 0.2915, 0.3, 0.2948, 0.2902, 0.3, 0.3, 0.3026, 0.2945, 0.3045, 0.2926, 0.3, 0.3042, 0.2902, 0.2958, 0.2907, 0.2918, 0.3, 0.2912, 0.3, 0.3, 0.2933, 0.2906, 0.3, 0.2935, 0.2924, 0.2904, 0.3037, 0.3, 0.2902, 0.2951, 0.3083, 0.2902, 0.2901, 0.2902, 0.3028, 0.3, 0.2954, 0.2952, 0.2909, 0.2912, 0.3067, 0.3037, 0.3068, 0.3, 0.291, 0.2926, 0.2901, 0.3, 0.2914, 0.2967, 0.2915, 0.3037, 0.2903, 0.2906, 0.3035, 0.3028, 0.3088, 0.3053, 0.309, 0.3018, 0.3071, 0.2909, 0.3041, 0.2912, 0.3064, 0.3022, 0.2946, 0.2942, 0.3065, 0.3031, 0.2952, 0.3051, 0.296, 0.3056, 0.3028, 0.2912, 0.2921, 0.3078, 0.3049, 0.2919, 0.3062, 0.2954, 0.2938, 0.2943, 0.3089, 0.2901, 0.2936, 0.3055, 0.2929, 0.3, 0.3, 0.3024, 0.3086, 0.3017, 0.3016, 0.3, 0.307, 0.3026, 0.2922, 0.304, 0.3057, 0.3027, 0.3031, 0.2919, 0.294, 0.3068, 0.2954, 0.3021, 0.3027, 0.2915, 0.2912, 0.2926, 0.3056, 0.291, 0.291, 0.3017, 0.2905, 0.2903, 0.3033, 0.3075, 0.3086, 0.3, 0.3088, 0.3023, 0.2959, 0.2936, 0.3037, 0.3088, 0.3088, 0.2937, 0.3035, 0.3053, 0.2915, 0.3, 0.2902, 0.2903, 0.309, 0.3023, 0.2958, 0.2902, 0.3045, 0.309, 0.3026, 0.2901, 0.2906, 0.2903, 0.3022, 0.2907, 0.3089, 0.2952, 0.2932, 0.3021, 0.2921, 0.3037, 0.2924, 0.3025, 0.3018, 0.3, 0.303, 0.2907, 0.3028, 0.3, 0.2909, 0.2914, 0.293, 0.3088, 0.2904, 0.2901, 0.308, 0.3023, 0.3, 0.2902, 0.3026, 0.3, 0.2931, 0.3035, 0.2945, 0.3088, 0.3, 0.3021, 0.2913, 0.3072, 0.3088, 0.2902, 0.3, 0.3074, 0.3, 0.3068, 0.3, 0.3092, 0.3, 0.296, 0.3026, 0.2937, 0.3029, 0.2929, 0.3065, 0.2907, 0.3, 0.3088, 0.2904, 0.3057, 0.3024, 0.3024, 0.3018, 0.2918, 0.2929, 0.3053, 0.308, 0.2906, 0.2902, 0.3, 0.2956, 0.2962, 0.3075, 0.3077, 0.2909, 0.2915, 0.2902, 0.291, 0.3, 0.3, 0.2932, 0.3053, 0.3021, 0.2922, 0.2939, 0.2903, 0.3024, 0.2942, 0.2926, 0.3, 0.2903, 0.3, 0.3017, 0.291, 0.295, 0.2903, 0.2922, 0.3035, 0.2945, 0.3024, 0.3084, 0.302, 0.3092, 0.3023, 0.2943, 0.3, 0.3086, 0.2931, 0.2911, 0.2946, 0.2905, 0.3042, 0.3, 0.305, 0.2916, 0.3053, 0.3, 0.2967, 0.3017, 0.2903, 0.2915, 0.3069, 0.294, 0.3048, 0.3, 0.2902, 0.3023, 0.309, 0.3, 0.2918, 0.3043, 0.2909, 0.2949, 0.3091, 0.3042, 0.2912, 0.302, 0.3, 0.2942, 0.3029, 0.2957, 0.3, 0.3088, 0.3, 0.3, 0.2936, 0.2956, 0.2953, 0.297, 0.305, 0.3, 0.2902, 0.3077, 0.2957, 0.306, 0.3, 0.3048, 0.3092, 0.2927, 0.3093, 0.3032, 0.3, 0.2956, 0.3021, 0.3092, 0.2912, 0.3061, 0.3056, 0.2945, 0.291, 0.3049], score: 0.5697\n",
      "    Epoch: 0, Time: 13.5831 s\n",
      "    Hyper parameters: [0.2811, 0.3185, 0.3154, 0.3086, 0.2844, 0.3071, 0.3058, 0.3, 0.3098, 0.2811, 0.3068, 0.3159, 0.3, 0.3116, 0.3151, 0.3166, 0.3081, 0.2819, 0.3, 0.3098, 0.2831, 0.3116, 0.2811, 0.2971, 0.3, 0.3051, 0.3069, 0.2808, 0.2819, 0.2997, 0.2885, 0.3073, 0.2816, 0.2811, 0.2853, 0.3132, 0.2822, 0.3071, 0.2805, 0.3104, 0.3, 0.3134, 0.2921, 0.281, 0.2881, 0.2865, 0.2829, 0.3141, 0.2808, 0.3168, 0.3, 0.2883, 0.2938, 0.3044, 0.3138, 0.2812, 0.3054, 0.3, 0.2929, 0.2826, 0.2863, 0.3, 0.3132, 0.2845, 0.2826, 0.2878, 0.3143, 0.2855, 0.2844, 0.3, 0.2861, 0.3064, 0.2861, 0.2822, 0.2882, 0.2829, 0.2823, 0.2822, 0.2818, 0.311, 0.3156, 0.2841, 0.3, 0.3, 0.3184, 0.2828, 0.3, 0.2885, 0.2811, 0.3, 0.3, 0.3062, 0.2878, 0.3109, 0.2845, 0.3, 0.31, 0.281, 0.2906, 0.2811, 0.2829, 0.3, 0.2823, 0.3, 0.3, 0.2855, 0.2809, 0.3, 0.286, 0.2957, 0.2809, 0.3087, 0.3, 0.2811, 0.2918, 0.3139, 0.2802, 0.2808, 0.2811, 0.3065, 0.3, 0.2897, 0.289, 0.2818, 0.2823, 0.3154, 0.3085, 0.3155, 0.3, 0.2819, 0.2847, 0.2809, 0.3, 0.2824, 0.2945, 0.2826, 0.3091, 0.2805, 0.281, 0.3083, 0.3066, 0.3118, 0.3127, 0.3067, 0.3045, 0.3154, 0.2818, 0.3101, 0.2818, 0.3141, 0.3053, 0.2881, 0.287, 0.3151, 0.3074, 0.2891, 0.3116, 0.2908, 0.3125, 0.3065, 0.2821, 0.2837, 0.3166, 0.3118, 0.283, 0.3146, 0.2895, 0.2863, 0.2876, 0.3156, 0.2809, 0.2862, 0.313, 0.2847, 0.3, 0.3, 0.3056, 0.3087, 0.3041, 0.3039, 0.3, 0.3151, 0.3061, 0.2839, 0.3092, 0.3134, 0.3064, 0.3075, 0.2828, 0.287, 0.3155, 0.2896, 0.305, 0.3066, 0.2827, 0.2823, 0.2845, 0.3132, 0.2819, 0.2818, 0.304, 0.2812, 0.2812, 0.3082, 0.316, 0.3179, 0.3, 0.3159, 0.3054, 0.2905, 0.286, 0.3085, 0.3162, 0.3182, 0.2864, 0.3087, 0.3127, 0.2826, 0.3, 0.281, 0.2808, 0.3129, 0.3055, 0.2907, 0.2811, 0.3104, 0.309, 0.3063, 0.2808, 0.2814, 0.2812, 0.3055, 0.2816, 0.3149, 0.2893, 0.2846, 0.3052, 0.2837, 0.3091, 0.2841, 0.306, 0.3044, 0.3, 0.3074, 0.2814, 0.3067, 0.3, 0.2818, 0.2825, 0.2869, 0.3066, 0.2808, 0.2809, 0.3152, 0.3054, 0.3, 0.2804, 0.3064, 0.3, 0.2846, 0.3087, 0.2876, 0.3118, 0.3, 0.305, 0.2825, 0.316, 0.3172, 0.2811, 0.3, 0.3153, 0.3, 0.3153, 0.3, 0.3139, 0.3, 0.2907, 0.3063, 0.2873, 0.3068, 0.2844, 0.3151, 0.2811, 0.3, 0.3163, 0.2813, 0.3127, 0.3056, 0.3058, 0.3042, 0.2832, 0.2848, 0.3119, 0.3171, 0.281, 0.281, 0.3, 0.2901, 0.2897, 0.3165, 0.3167, 0.2818, 0.2827, 0.281, 0.282, 0.3, 0.3, 0.2856, 0.3119, 0.3049, 0.2835, 0.2867, 0.2812, 0.3057, 0.2871, 0.2842, 0.3, 0.2879, 0.3, 0.3041, 0.2819, 0.2886, 0.2812, 0.2838, 0.3082, 0.2876, 0.3058, 0.315, 0.3048, 0.3188, 0.3054, 0.2875, 0.3, 0.316, 0.2851, 0.282, 0.2882, 0.2816, 0.3097, 0.3, 0.3113, 0.29, 0.312, 0.3, 0.2922, 0.2996, 0.2813, 0.2828, 0.3156, 0.287, 0.311, 0.3, 0.281, 0.3054, 0.317, 0.3, 0.2832, 0.3104, 0.2818, 0.2885, 0.3061, 0.3096, 0.2823, 0.3048, 0.3, 0.2873, 0.3067, 0.2877, 0.3, 0.3182, 0.3, 0.3, 0.2853, 0.2901, 0.2893, 0.297, 0.3114, 0.3, 0.2803, 0.3167, 0.2901, 0.3133, 0.3, 0.3116, 0.3135, 0.2848, 0.3169, 0.3075, 0.3, 0.2899, 0.3049, 0.3188, 0.2823, 0.3135, 0.3125, 0.2867, 0.2818, 0.3119], score: 0.5740\n",
      "    Epoch: 1, Time: 14.1262 s\n",
      "    Hyper parameters: [0.2725, 0.3267, 0.3243, 0.3049, 0.2759, 0.3119, 0.3098, 0.3, 0.307, 0.2725, 0.3115, 0.313, 0.3, 0.3191, 0.324, 0.3216, 0.3039, 0.2728, 0.3, 0.3166, 0.2745, 0.3115, 0.2726, 0.3035, 0.3, 0.3087, 0.3116, 0.2722, 0.2731, 0.3017, 0.2797, 0.3125, 0.2733, 0.2767, 0.2776, 0.3146, 0.2733, 0.3118, 0.2706, 0.3176, 0.3, 0.3219, 0.2897, 0.2724, 0.281, 0.2784, 0.274, 0.3207, 0.2723, 0.3244, 0.3, 0.2812, 0.2988, 0.3076, 0.3226, 0.2726, 0.3092, 0.3, 0.2883, 0.2737, 0.2785, 0.3, 0.3219, 0.2763, 0.2734, 0.2805, 0.323, 0.2768, 0.2761, 0.3, 0.2782, 0.3107, 0.2771, 0.2733, 0.2812, 0.2742, 0.2734, 0.2733, 0.2727, 0.3106, 0.3229, 0.2757, 0.3, 0.3, 0.3238, 0.274, 0.3, 0.2815, 0.2725, 0.3, 0.3, 0.3106, 0.2805, 0.3182, 0.2761, 0.3, 0.3167, 0.2724, 0.2847, 0.2717, 0.2738, 0.3, 0.2736, 0.3, 0.3, 0.2773, 0.2715, 0.3, 0.288, 0.2956, 0.2711, 0.3136, 0.3, 0.2721, 0.2892, 0.3183, 0.2709, 0.2721, 0.2725, 0.311, 0.3, 0.2834, 0.2823, 0.2729, 0.2822, 0.3233, 0.3146, 0.3222, 0.3, 0.2731, 0.2764, 0.2723, 0.3, 0.2745, 0.2928, 0.2742, 0.3155, 0.2712, 0.2717, 0.3137, 0.3111, 0.312, 0.321, 0.3013, 0.3078, 0.3243, 0.2729, 0.3171, 0.2723, 0.3227, 0.3091, 0.2809, 0.2794, 0.3162, 0.3126, 0.2824, 0.3191, 0.2841, 0.3205, 0.311, 0.2731, 0.2753, 0.3258, 0.3195, 0.274, 0.3211, 0.293, 0.2784, 0.2805, 0.3215, 0.2722, 0.2783, 0.3214, 0.2763, 0.3, 0.3, 0.3097, 0.3055, 0.307, 0.3067, 0.3, 0.324, 0.3102, 0.2755, 0.3155, 0.3219, 0.3109, 0.3128, 0.2737, 0.2795, 0.3192, 0.2832, 0.3085, 0.3112, 0.2739, 0.2737, 0.2849, 0.3131, 0.2731, 0.2728, 0.3068, 0.2721, 0.2726, 0.3139, 0.3251, 0.3261, 0.3, 0.323, 0.3091, 0.2847, 0.278, 0.3146, 0.3232, 0.3243, 0.2879, 0.3148, 0.3213, 0.2737, 0.3, 0.2724, 0.2713, 0.3125, 0.3095, 0.2848, 0.2725, 0.3174, 0.3055, 0.3109, 0.2722, 0.2721, 0.2721, 0.3095, 0.2726, 0.3195, 0.2821, 0.2759, 0.309, 0.2753, 0.3155, 0.2756, 0.3101, 0.3074, 0.3, 0.3125, 0.2721, 0.3115, 0.3, 0.273, 0.2735, 0.2797, 0.3021, 0.2714, 0.2723, 0.3228, 0.3091, 0.3, 0.2752, 0.311, 0.3, 0.2759, 0.3148, 0.2923, 0.3116, 0.3, 0.3085, 0.2739, 0.3158, 0.3264, 0.272, 0.3, 0.324, 0.3, 0.3243, 0.3, 0.3161, 0.3, 0.2866, 0.3109, 0.2807, 0.3118, 0.2857, 0.3232, 0.2718, 0.3, 0.3237, 0.2724, 0.3209, 0.3095, 0.3101, 0.3072, 0.2746, 0.2763, 0.3201, 0.3266, 0.2715, 0.2724, 0.3, 0.284, 0.2832, 0.3215, 0.3258, 0.273, 0.2739, 0.2724, 0.2733, 0.3, 0.3, 0.2776, 0.3197, 0.3084, 0.2748, 0.279, 0.2777, 0.3098, 0.2868, 0.2756, 0.3, 0.2887, 0.3, 0.307, 0.2731, 0.2837, 0.2727, 0.2753, 0.3137, 0.2803, 0.3098, 0.321, 0.3082, 0.3216, 0.3093, 0.2801, 0.3, 0.3229, 0.2768, 0.2729, 0.2813, 0.2732, 0.3164, 0.3, 0.3189, 0.2861, 0.3199, 0.3, 0.2888, 0.299, 0.2775, 0.2738, 0.3238, 0.2796, 0.3183, 0.3, 0.2724, 0.3092, 0.3248, 0.3, 0.2746, 0.3175, 0.2729, 0.2815, 0.3002, 0.3158, 0.2736, 0.3082, 0.3, 0.288, 0.3113, 0.2788, 0.3, 0.323, 0.3, 0.3, 0.2768, 0.2825, 0.2846, 0.2984, 0.3189, 0.3, 0.2711, 0.3242, 0.2858, 0.3217, 0.3, 0.3192, 0.3153, 0.2769, 0.3241, 0.3125, 0.3, 0.2836, 0.3083, 0.3176, 0.2737, 0.3221, 0.3205, 0.2784, 0.2795, 0.3203], score: 0.5762\n",
      "    Epoch: 2, Time: 14.3147 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Hyper parameters: [0.2644, 0.3353, 0.3317, 0.3051, 0.2676, 0.3174, 0.3144, 0.3, 0.3028, 0.2682, 0.3169, 0.3127, 0.3, 0.3276, 0.332, 0.3247, 0.298, 0.264, 0.3, 0.3244, 0.266, 0.3095, 0.2645, 0.3063, 0.3, 0.3129, 0.3169, 0.2635, 0.2645, 0.3019, 0.271, 0.3187, 0.2654, 0.2742, 0.2729, 0.314, 0.2645, 0.3173, 0.2614, 0.3256, 0.3, 0.3307, 0.2877, 0.2643, 0.2736, 0.2742, 0.2653, 0.3275, 0.2663, 0.3325, 0.3, 0.2738, 0.3053, 0.3114, 0.3298, 0.2646, 0.3136, 0.3, 0.2832, 0.2648, 0.2705, 0.3, 0.3311, 0.2681, 0.2643, 0.273, 0.3234, 0.2685, 0.2677, 0.3, 0.2703, 0.3158, 0.2683, 0.2645, 0.2738, 0.2701, 0.2646, 0.2645, 0.2639, 0.3085, 0.3293, 0.2667, 0.3, 0.3, 0.3267, 0.2654, 0.3, 0.2743, 0.2644, 0.3, 0.3, 0.3157, 0.273, 0.3265, 0.2676, 0.3, 0.3241, 0.2643, 0.2784, 0.2627, 0.2648, 0.3, 0.2651, 0.3, 0.3, 0.2692, 0.2627, 0.3, 0.2883, 0.2934, 0.2617, 0.3194, 0.3, 0.2634, 0.2872, 0.3218, 0.2621, 0.2634, 0.2644, 0.3162, 0.3, 0.2781, 0.2752, 0.2643, 0.2856, 0.3311, 0.3214, 0.3277, 0.3, 0.2639, 0.268, 0.2631, 0.3, 0.2662, 0.2914, 0.2654, 0.323, 0.2625, 0.2628, 0.32, 0.3163, 0.3104, 0.3299, 0.2954, 0.3116, 0.3337, 0.2643, 0.325, 0.2629, 0.3316, 0.3135, 0.2735, 0.2716, 0.3138, 0.3184, 0.2749, 0.3276, 0.2772, 0.3292, 0.3162, 0.2643, 0.267, 0.3316, 0.328, 0.2652, 0.3249, 0.299, 0.2704, 0.273, 0.3271, 0.2641, 0.2701, 0.3303, 0.2757, 0.3, 0.3, 0.3145, 0.2997, 0.3103, 0.3098, 0.3, 0.3334, 0.3151, 0.2673, 0.3225, 0.3308, 0.3162, 0.319, 0.2651, 0.2718, 0.3207, 0.2779, 0.3125, 0.3166, 0.2652, 0.2652, 0.2885, 0.3096, 0.2646, 0.2639, 0.31, 0.2634, 0.2645, 0.3205, 0.3303, 0.3339, 0.3, 0.3306, 0.3134, 0.2774, 0.2699, 0.3214, 0.3304, 0.328, 0.2925, 0.3218, 0.3245, 0.2647, 0.3, 0.264, 0.2652, 0.3097, 0.314, 0.2785, 0.2641, 0.3251, 0.3002, 0.316, 0.264, 0.2628, 0.2635, 0.314, 0.2639, 0.3233, 0.2783, 0.2674, 0.3133, 0.2668, 0.323, 0.2671, 0.315, 0.3109, 0.3, 0.3186, 0.263, 0.3169, 0.3, 0.2644, 0.2645, 0.2721, 0.3012, 0.2625, 0.2631, 0.3313, 0.3133, 0.3, 0.2728, 0.3165, 0.3, 0.2675, 0.3218, 0.299, 0.3093, 0.3, 0.3126, 0.2654, 0.3122, 0.3257, 0.2634, 0.3, 0.3328, 0.3, 0.3329, 0.3, 0.3166, 0.3, 0.2833, 0.3161, 0.2734, 0.3175, 0.2901, 0.3315, 0.2629, 0.3, 0.3316, 0.2638, 0.3297, 0.3141, 0.315, 0.3106, 0.2662, 0.2677, 0.3267, 0.333, 0.2624, 0.2643, 0.3, 0.2789, 0.2769, 0.3241, 0.3352, 0.264, 0.2652, 0.2643, 0.2647, 0.3, 0.3, 0.2694, 0.328, 0.3125, 0.2661, 0.271, 0.2772, 0.3145, 0.2897, 0.2672, 0.3, 0.2916, 0.3, 0.3105, 0.2646, 0.2796, 0.2647, 0.2668, 0.32, 0.2818, 0.3145, 0.3269, 0.3121, 0.3208, 0.3137, 0.2724, 0.3, 0.33, 0.2683, 0.264, 0.2741, 0.2652, 0.3242, 0.3, 0.3273, 0.2808, 0.3285, 0.3, 0.286, 0.2993, 0.2768, 0.2648, 0.3319, 0.2719, 0.3265, 0.3, 0.2643, 0.3136, 0.333, 0.3, 0.2661, 0.3256, 0.2643, 0.2757, 0.2977, 0.3231, 0.2651, 0.3121, 0.3, 0.2918, 0.3166, 0.2701, 0.3, 0.3245, 0.3, 0.3, 0.2681, 0.2744, 0.2808, 0.301, 0.3273, 0.3, 0.2625, 0.331, 0.2823, 0.3306, 0.3, 0.3276, 0.3153, 0.2688, 0.3315, 0.3183, 0.3, 0.2764, 0.3123, 0.3132, 0.265, 0.3312, 0.3292, 0.2754, 0.2807, 0.3293], score: 0.5778\n",
      "    Epoch: 3, Time: 14.6858 s\n",
      "    Hyper parameters: [0.2567, 0.3442, 0.3379, 0.3077, 0.2595, 0.3235, 0.3196, 0.3, 0.3026, 0.2656, 0.323, 0.3141, 0.3, 0.3363, 0.3395, 0.3265, 0.2957, 0.2555, 0.3, 0.3329, 0.2575, 0.3063, 0.2569, 0.3067, 0.3, 0.3177, 0.3228, 0.2548, 0.2562, 0.3008, 0.2658, 0.3254, 0.2578, 0.2731, 0.2676, 0.3121, 0.2559, 0.3234, 0.2555, 0.3342, 0.3, 0.339, 0.286, 0.2566, 0.2661, 0.2725, 0.2566, 0.3347, 0.2605, 0.3413, 0.3, 0.2676, 0.3085, 0.3158, 0.336, 0.257, 0.3185, 0.3, 0.2778, 0.2606, 0.2622, 0.3, 0.3388, 0.2599, 0.2553, 0.2654, 0.3203, 0.2605, 0.2592, 0.3, 0.262, 0.3214, 0.2625, 0.2559, 0.2664, 0.2652, 0.2612, 0.2558, 0.2584, 0.305, 0.3353, 0.2574, 0.3, 0.3, 0.3272, 0.2569, 0.3, 0.267, 0.2566, 0.3, 0.3, 0.3214, 0.2653, 0.3352, 0.259, 0.3, 0.3321, 0.2596, 0.2719, 0.2539, 0.256, 0.3, 0.2625, 0.3, 0.3, 0.2611, 0.2544, 0.3, 0.2872, 0.295, 0.2527, 0.3261, 0.3, 0.2552, 0.2854, 0.3248, 0.254, 0.2552, 0.2568, 0.322, 0.3, 0.2737, 0.2678, 0.2558, 0.2908, 0.3392, 0.3291, 0.3324, 0.3, 0.2546, 0.2598, 0.2541, 0.3, 0.2582, 0.2903, 0.2569, 0.3314, 0.2544, 0.2544, 0.327, 0.3221, 0.3075, 0.3359, 0.2928, 0.3158, 0.3315, 0.256, 0.3336, 0.2539, 0.3391, 0.3183, 0.2743, 0.2636, 0.3095, 0.325, 0.2672, 0.3363, 0.2703, 0.3359, 0.322, 0.2559, 0.2588, 0.3345, 0.3368, 0.2566, 0.3266, 0.3032, 0.2622, 0.2656, 0.3324, 0.2562, 0.2618, 0.3368, 0.2785, 0.3, 0.3, 0.3196, 0.2974, 0.314, 0.3135, 0.3, 0.3336, 0.3205, 0.259, 0.3303, 0.3358, 0.3221, 0.3257, 0.2569, 0.264, 0.3208, 0.2805, 0.317, 0.3226, 0.2567, 0.2571, 0.2899, 0.3089, 0.2563, 0.2554, 0.3136, 0.2549, 0.2568, 0.3277, 0.3323, 0.342, 0.3, 0.338, 0.3183, 0.2704, 0.2618, 0.3291, 0.3381, 0.33, 0.2987, 0.3298, 0.3238, 0.2557, 0.3, 0.256, 0.2616, 0.3063, 0.3191, 0.2732, 0.2561, 0.3336, 0.2988, 0.3218, 0.2561, 0.2538, 0.2553, 0.319, 0.2551, 0.3264, 0.2769, 0.2591, 0.3181, 0.2583, 0.3314, 0.2586, 0.3204, 0.3148, 0.3, 0.3252, 0.2542, 0.3231, 0.3, 0.2561, 0.2556, 0.265, 0.3027, 0.2541, 0.2541, 0.3354, 0.3182, 0.3, 0.2724, 0.3225, 0.3, 0.2594, 0.3295, 0.3042, 0.3059, 0.3, 0.3171, 0.257, 0.3115, 0.3218, 0.2552, 0.3, 0.3417, 0.3, 0.3412, 0.3, 0.316, 0.3, 0.2804, 0.3219, 0.2662, 0.324, 0.2962, 0.3405, 0.2545, 0.3, 0.3402, 0.2557, 0.3275, 0.3191, 0.3209, 0.3144, 0.2582, 0.2592, 0.3323, 0.3371, 0.261, 0.2566, 0.3, 0.2724, 0.2698, 0.3248, 0.3388, 0.2549, 0.2567, 0.2567, 0.2565, 0.3, 0.3, 0.2684, 0.3369, 0.317, 0.2576, 0.2642, 0.2789, 0.3197, 0.2944, 0.2585, 0.3, 0.2958, 0.3, 0.3143, 0.2619, 0.2762, 0.2571, 0.2584, 0.3272, 0.282, 0.3198, 0.3327, 0.3165, 0.3179, 0.3186, 0.2646, 0.3, 0.3377, 0.2598, 0.2554, 0.2668, 0.2576, 0.3326, 0.3, 0.3274, 0.2777, 0.3364, 0.3, 0.2836, 0.3005, 0.2783, 0.256, 0.3401, 0.2641, 0.3353, 0.3, 0.2567, 0.3187, 0.3418, 0.3, 0.2574, 0.3342, 0.2558, 0.2709, 0.2975, 0.3313, 0.257, 0.3166, 0.3, 0.2973, 0.3224, 0.2617, 0.3, 0.3236, 0.3, 0.3, 0.2598, 0.2698, 0.2776, 0.3017, 0.336, 0.3, 0.2542, 0.3372, 0.2793, 0.3337, 0.3, 0.3364, 0.3139, 0.2607, 0.3395, 0.3247, 0.3, 0.2683, 0.3167, 0.3071, 0.2566, 0.3389, 0.3375, 0.2721, 0.284, 0.3382], score: 0.5791\n",
      "    Epoch: 4, Time: 14.7885 s\n",
      "    Hyper parameters: [0.2495, 0.35, 0.3426, 0.3076, 0.2515, 0.3303, 0.3252, 0.3, 0.3048, 0.2645, 0.3297, 0.3168, 0.3, 0.3395, 0.3467, 0.3273, 0.296, 0.2472, 0.3, 0.3402, 0.2491, 0.3069, 0.2496, 0.3054, 0.3, 0.3231, 0.3293, 0.2464, 0.2482, 0.2987, 0.2623, 0.3328, 0.2548, 0.273, 0.2642, 0.3093, 0.2474, 0.33, 0.2512, 0.3428, 0.3, 0.3471, 0.2846, 0.2494, 0.2585, 0.2727, 0.2481, 0.3429, 0.255, 0.3496, 0.3, 0.2622, 0.3094, 0.3206, 0.3416, 0.2497, 0.3239, 0.3, 0.2722, 0.2595, 0.2538, 0.3, 0.3455, 0.2518, 0.2464, 0.2578, 0.3154, 0.2528, 0.2509, 0.3, 0.2534, 0.3276, 0.2584, 0.2475, 0.2589, 0.2587, 0.2597, 0.2471, 0.2543, 0.3007, 0.3409, 0.2481, 0.3, 0.3, 0.3261, 0.2486, 0.3, 0.2592, 0.2491, 0.3, 0.3, 0.3277, 0.2576, 0.3404, 0.2505, 0.3, 0.3402, 0.2575, 0.2651, 0.2491, 0.2473, 0.3, 0.2631, 0.3, 0.3, 0.2531, 0.2466, 0.3, 0.2897, 0.2949, 0.2442, 0.3333, 0.3, 0.2501, 0.2839, 0.3274, 0.2463, 0.2476, 0.2495, 0.3284, 0.3, 0.2699, 0.2614, 0.2521, 0.2972, 0.348, 0.3375, 0.3364, 0.3, 0.2454, 0.2517, 0.2454, 0.3, 0.2501, 0.2893, 0.2484, 0.3333, 0.2468, 0.2465, 0.3346, 0.3284, 0.3037, 0.3396, 0.294, 0.3204, 0.327, 0.2478, 0.3426, 0.2454, 0.3457, 0.3237, 0.2751, 0.2562, 0.3083, 0.332, 0.2607, 0.3455, 0.2634, 0.3408, 0.3284, 0.247, 0.2507, 0.3353, 0.3444, 0.2482, 0.3269, 0.3058, 0.2539, 0.2574, 0.3377, 0.2487, 0.2547, 0.3415, 0.2797, 0.3, 0.3, 0.3254, 0.2975, 0.3181, 0.3177, 0.3, 0.3305, 0.3265, 0.2508, 0.3387, 0.3375, 0.3285, 0.3331, 0.2505, 0.2562, 0.3197, 0.2856, 0.3219, 0.3291, 0.2484, 0.2491, 0.2898, 0.31, 0.2483, 0.2466, 0.3175, 0.2462, 0.2493, 0.3356, 0.332, 0.3506, 0.3, 0.3455, 0.3235, 0.2635, 0.2538, 0.3375, 0.3466, 0.3306, 0.3026, 0.3384, 0.3212, 0.2469, 0.3, 0.2484, 0.2597, 0.3068, 0.3246, 0.2687, 0.2485, 0.3423, 0.2998, 0.3281, 0.2486, 0.2445, 0.2475, 0.3246, 0.2462, 0.3292, 0.2773, 0.2502, 0.3235, 0.25, 0.3403, 0.2502, 0.3264, 0.3192, 0.3, 0.3325, 0.2456, 0.3298, 0.3, 0.2481, 0.247, 0.2657, 0.3059, 0.2455, 0.2455, 0.3365, 0.3237, 0.3, 0.2736, 0.3292, 0.3, 0.2516, 0.3379, 0.308, 0.3017, 0.3, 0.3223, 0.2488, 0.3129, 0.3161, 0.2474, 0.3, 0.3495, 0.3, 0.3497, 0.3, 0.3144, 0.3, 0.278, 0.3282, 0.26, 0.3311, 0.2971, 0.3494, 0.2464, 0.3, 0.3493, 0.2479, 0.3236, 0.3247, 0.3273, 0.3187, 0.2503, 0.2507, 0.3375, 0.3392, 0.263, 0.2494, 0.3, 0.2648, 0.263, 0.3242, 0.337, 0.246, 0.2483, 0.2494, 0.2484, 0.3, 0.3, 0.2708, 0.3459, 0.322, 0.2494, 0.2583, 0.2821, 0.3255, 0.2968, 0.2499, 0.3, 0.2958, 0.3, 0.3186, 0.2625, 0.2733, 0.2499, 0.2564, 0.3349, 0.2812, 0.3259, 0.3387, 0.3212, 0.3135, 0.324, 0.2579, 0.3, 0.3461, 0.2514, 0.2469, 0.2594, 0.2503, 0.3415, 0.3, 0.3241, 0.2737, 0.3438, 0.3, 0.2815, 0.2994, 0.2812, 0.2474, 0.3481, 0.2563, 0.3444, 0.3, 0.2493, 0.3245, 0.3494, 0.3, 0.2487, 0.3354, 0.2475, 0.2667, 0.299, 0.3305, 0.2491, 0.3216, 0.3, 0.3006, 0.3288, 0.2537, 0.3, 0.3212, 0.3, 0.3, 0.2522, 0.2646, 0.2748, 0.3009, 0.3444, 0.3, 0.2464, 0.3432, 0.2767, 0.3325, 0.3, 0.3354, 0.3118, 0.2527, 0.3479, 0.3317, 0.3, 0.2599, 0.3215, 0.3032, 0.2479, 0.3456, 0.3458, 0.2686, 0.2887, 0.3475], score: 0.5811\n",
      "    Epoch: 5, Time: 15.1814 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Hyper parameters: [0.2427, 0.3535, 0.3463, 0.3058, 0.2437, 0.3378, 0.3313, 0.3, 0.3087, 0.2644, 0.3374, 0.3156, 0.3, 0.3375, 0.3541, 0.3273, 0.2979, 0.2415, 0.3, 0.3453, 0.2409, 0.31, 0.2414, 0.3028, 0.3, 0.329, 0.3364, 0.2382, 0.2404, 0.2958, 0.26, 0.3411, 0.2549, 0.2737, 0.2601, 0.3059, 0.239, 0.3373, 0.2458, 0.3516, 0.3, 0.3552, 0.2833, 0.2424, 0.2508, 0.2744, 0.2397, 0.3516, 0.2496, 0.3578, 0.3, 0.2575, 0.3085, 0.3262, 0.3468, 0.2427, 0.3297, 0.3, 0.2664, 0.2606, 0.2457, 0.3, 0.3513, 0.2437, 0.2428, 0.2501, 0.3129, 0.2453, 0.2427, 0.3, 0.2448, 0.3342, 0.2554, 0.2393, 0.2514, 0.2525, 0.2596, 0.2384, 0.2516, 0.2957, 0.3464, 0.2388, 0.3, 0.3, 0.3236, 0.2404, 0.3, 0.2506, 0.242, 0.3, 0.3, 0.3346, 0.2498, 0.343, 0.2522, 0.3, 0.3484, 0.2564, 0.2593, 0.2471, 0.2388, 0.3, 0.2659, 0.3, 0.3, 0.2469, 0.2393, 0.3, 0.291, 0.2934, 0.2352, 0.3411, 0.3, 0.2465, 0.2825, 0.3296, 0.2392, 0.2405, 0.2425, 0.3356, 0.3, 0.2666, 0.2559, 0.2516, 0.301, 0.3571, 0.3416, 0.3399, 0.3, 0.2362, 0.2438, 0.2371, 0.3, 0.2422, 0.2884, 0.2401, 0.3318, 0.2397, 0.239, 0.3428, 0.3352, 0.3038, 0.3416, 0.2965, 0.3254, 0.3222, 0.2399, 0.352, 0.2373, 0.3513, 0.3297, 0.2757, 0.2481, 0.3092, 0.3397, 0.2542, 0.3522, 0.2565, 0.3457, 0.3356, 0.238, 0.2427, 0.3345, 0.3511, 0.2482, 0.3259, 0.3073, 0.2457, 0.249, 0.343, 0.2415, 0.2485, 0.3446, 0.2798, 0.3, 0.3, 0.3316, 0.2994, 0.3225, 0.3223, 0.3, 0.3262, 0.3329, 0.2423, 0.3474, 0.3368, 0.3355, 0.3411, 0.2443, 0.2484, 0.3176, 0.29, 0.3274, 0.3361, 0.2403, 0.2408, 0.2883, 0.3126, 0.2405, 0.2379, 0.3218, 0.2372, 0.2422, 0.3441, 0.33, 0.3597, 0.3, 0.3537, 0.3292, 0.256, 0.2527, 0.3464, 0.3555, 0.3302, 0.3049, 0.3473, 0.3173, 0.2382, 0.3, 0.2411, 0.2587, 0.3097, 0.3306, 0.2647, 0.2412, 0.3454, 0.3026, 0.335, 0.2414, 0.2353, 0.2401, 0.3306, 0.2374, 0.3316, 0.279, 0.2414, 0.3294, 0.2419, 0.3492, 0.2419, 0.3333, 0.3239, 0.3, 0.3404, 0.2374, 0.337, 0.3, 0.2397, 0.2385, 0.2693, 0.3102, 0.2367, 0.2374, 0.3356, 0.3299, 0.3, 0.2759, 0.3364, 0.3, 0.244, 0.3466, 0.3107, 0.3014, 0.3, 0.3278, 0.2409, 0.3159, 0.3124, 0.24, 0.3, 0.3562, 0.3, 0.3578, 0.3, 0.312, 0.3, 0.2759, 0.3351, 0.2546, 0.339, 0.297, 0.3587, 0.2387, 0.3, 0.3524, 0.2406, 0.3216, 0.3308, 0.3345, 0.3232, 0.2424, 0.2422, 0.3428, 0.3399, 0.2671, 0.2424, 0.3, 0.2568, 0.2564, 0.3225, 0.3338, 0.2375, 0.2401, 0.2423, 0.2406, 0.3, 0.3, 0.2751, 0.3452, 0.3274, 0.2414, 0.2532, 0.2838, 0.332, 0.2975, 0.2416, 0.3, 0.2934, 0.3, 0.3232, 0.2653, 0.2707, 0.243, 0.2578, 0.3433, 0.2796, 0.3325, 0.3449, 0.3264, 0.3121, 0.3298, 0.2521, 0.3, 0.355, 0.244, 0.2386, 0.2529, 0.2433, 0.35, 0.3, 0.3231, 0.2711, 0.351, 0.3, 0.2797, 0.2992, 0.2852, 0.2391, 0.3561, 0.2494, 0.3531, 0.3, 0.2423, 0.3308, 0.356, 0.3, 0.2403, 0.3332, 0.2464, 0.263, 0.3018, 0.3275, 0.2462, 0.327, 0.3, 0.3022, 0.3357, 0.2462, 0.3, 0.3178, 0.3, 0.3, 0.2452, 0.2591, 0.2724, 0.299, 0.3529, 0.3, 0.239, 0.3489, 0.2744, 0.3296, 0.3, 0.3314, 0.3089, 0.2449, 0.3568, 0.3394, 0.3, 0.2517, 0.3268, 0.3011, 0.2388, 0.3514, 0.3544, 0.2669, 0.2914, 0.3464], score: 0.5823\n",
      "    Epoch: 6, Time: 14.8119 s\n",
      "    Hyper parameters: [0.2361, 0.3544, 0.349, 0.306, 0.2369, 0.3458, 0.3379, 0.3, 0.3108, 0.2652, 0.3456, 0.3121, 0.3, 0.3341, 0.362, 0.3265, 0.3013, 0.2372, 0.3, 0.3486, 0.2329, 0.3115, 0.2331, 0.2992, 0.3, 0.3356, 0.3439, 0.2305, 0.2328, 0.2923, 0.2588, 0.3498, 0.257, 0.2752, 0.2555, 0.3018, 0.2308, 0.3451, 0.2395, 0.3608, 0.3, 0.3637, 0.2822, 0.2358, 0.2441, 0.2772, 0.2323, 0.3599, 0.2444, 0.3659, 0.3, 0.2533, 0.3063, 0.3324, 0.3517, 0.236, 0.336, 0.3, 0.2614, 0.2632, 0.2377, 0.3, 0.3565, 0.2357, 0.2425, 0.2433, 0.3123, 0.2381, 0.2346, 0.3, 0.2365, 0.3415, 0.2535, 0.2313, 0.2447, 0.2466, 0.2607, 0.2298, 0.2496, 0.2946, 0.3518, 0.2305, 0.3, 0.3, 0.3205, 0.2323, 0.3, 0.242, 0.2352, 0.3, 0.3, 0.342, 0.243, 0.3432, 0.2566, 0.3, 0.3573, 0.2563, 0.2541, 0.2486, 0.2355, 0.3, 0.2701, 0.3, 0.3, 0.2406, 0.2347, 0.3, 0.291, 0.2946, 0.2263, 0.3494, 0.3, 0.2441, 0.2814, 0.3316, 0.2324, 0.2354, 0.2383, 0.3433, 0.3, 0.261, 0.251, 0.2531, 0.3028, 0.3657, 0.3428, 0.343, 0.3, 0.2272, 0.2361, 0.2293, 0.3, 0.2345, 0.2876, 0.2321, 0.3283, 0.233, 0.2318, 0.3516, 0.3426, 0.3063, 0.342, 0.3, 0.3308, 0.3168, 0.2313, 0.3603, 0.2325, 0.3563, 0.3361, 0.2763, 0.2394, 0.3118, 0.3479, 0.2467, 0.3569, 0.2504, 0.3491, 0.3433, 0.229, 0.2349, 0.3324, 0.3569, 0.2513, 0.324, 0.3078, 0.2385, 0.2407, 0.3483, 0.2347, 0.242, 0.3464, 0.2789, 0.3, 0.3, 0.3384, 0.3027, 0.3273, 0.3276, 0.3, 0.324, 0.34, 0.2337, 0.3533, 0.3345, 0.3433, 0.3496, 0.2382, 0.2406, 0.3148, 0.2932, 0.3333, 0.3435, 0.2318, 0.2321, 0.2859, 0.3125, 0.233, 0.2293, 0.3264, 0.2305, 0.2354, 0.3529, 0.3268, 0.3687, 0.3, 0.3625, 0.3355, 0.2563, 0.2548, 0.3543, 0.3567, 0.329, 0.3058, 0.3561, 0.3164, 0.2297, 0.3, 0.2342, 0.2584, 0.3115, 0.3371, 0.2612, 0.2343, 0.3452, 0.3067, 0.3426, 0.2345, 0.2265, 0.2332, 0.337, 0.2286, 0.3337, 0.2818, 0.2328, 0.3357, 0.234, 0.3585, 0.2345, 0.3407, 0.329, 0.3, 0.3489, 0.2295, 0.3449, 0.3, 0.2311, 0.2303, 0.2747, 0.3114, 0.228, 0.2295, 0.3331, 0.3373, 0.3, 0.2791, 0.3442, 0.3, 0.2357, 0.35, 0.3126, 0.3036, 0.3, 0.3339, 0.2332, 0.3161, 0.3104, 0.233, 0.3, 0.3621, 0.3, 0.3658, 0.3, 0.3092, 0.3, 0.274, 0.3427, 0.2498, 0.3475, 0.2961, 0.3597, 0.2313, 0.3, 0.3521, 0.2337, 0.3212, 0.3373, 0.3424, 0.3283, 0.234, 0.2347, 0.3479, 0.3392, 0.2701, 0.2358, 0.3, 0.2489, 0.2494, 0.32, 0.3295, 0.2294, 0.232, 0.2356, 0.2323, 0.3, 0.3, 0.2807, 0.3415, 0.3332, 0.2335, 0.2488, 0.2842, 0.3389, 0.2968, 0.2336, 0.3, 0.2892, 0.3, 0.3282, 0.2696, 0.2685, 0.2364, 0.2612, 0.3521, 0.2816, 0.34, 0.3516, 0.3319, 0.3127, 0.3361, 0.2469, 0.3, 0.3639, 0.2376, 0.2305, 0.2472, 0.2366, 0.3583, 0.3, 0.3239, 0.2697, 0.358, 0.3, 0.2781, 0.2996, 0.2877, 0.2337, 0.3647, 0.2434, 0.3617, 0.3, 0.2356, 0.338, 0.3619, 0.3, 0.2323, 0.3333, 0.2486, 0.2598, 0.3055, 0.3265, 0.2465, 0.3331, 0.3, 0.3024, 0.3432, 0.239, 0.3, 0.3138, 0.3, 0.3, 0.2384, 0.2533, 0.2702, 0.2962, 0.3614, 0.3, 0.2341, 0.3546, 0.2725, 0.3253, 0.3, 0.3292, 0.3054, 0.237, 0.3636, 0.3476, 0.3, 0.2438, 0.3324, 0.3004, 0.2297, 0.3565, 0.3635, 0.2665, 0.2921, 0.3428], score: 0.5835\n",
      "    Epoch: 7, Time: 14.7485 s\n",
      "    Hyper parameters: [0.2298, 0.3535, 0.3508, 0.3078, 0.2308, 0.3545, 0.3448, 0.3, 0.3112, 0.2667, 0.3543, 0.31, 0.3, 0.333, 0.3705, 0.3252, 0.3056, 0.234, 0.3, 0.3504, 0.2251, 0.3119, 0.225, 0.2985, 0.3, 0.3426, 0.3519, 0.2234, 0.2254, 0.2928, 0.2585, 0.3588, 0.2606, 0.2773, 0.2506, 0.3018, 0.2229, 0.3535, 0.2331, 0.3703, 0.3, 0.3726, 0.2812, 0.2281, 0.2381, 0.2808, 0.2293, 0.3681, 0.2401, 0.3743, 0.3, 0.2497, 0.303, 0.3397, 0.3565, 0.2296, 0.3428, 0.3, 0.2569, 0.2671, 0.2299, 0.3, 0.361, 0.2287, 0.2443, 0.2373, 0.3132, 0.2313, 0.2274, 0.3, 0.2285, 0.3493, 0.2522, 0.2235, 0.2378, 0.2406, 0.2628, 0.2214, 0.2483, 0.2959, 0.3571, 0.2232, 0.3, 0.3, 0.3168, 0.2245, 0.3, 0.2335, 0.2287, 0.3, 0.3, 0.3499, 0.237, 0.3416, 0.2616, 0.3, 0.3664, 0.2569, 0.2495, 0.2512, 0.2354, 0.3, 0.2755, 0.3, 0.3, 0.2342, 0.2324, 0.3, 0.2902, 0.2947, 0.2177, 0.3581, 0.3, 0.2428, 0.2803, 0.3334, 0.2277, 0.233, 0.2362, 0.3517, 0.3, 0.254, 0.2467, 0.2563, 0.3024, 0.3738, 0.3419, 0.3457, 0.3, 0.2224, 0.2285, 0.2218, 0.3, 0.2273, 0.2869, 0.2245, 0.3272, 0.2267, 0.225, 0.3604, 0.3506, 0.3105, 0.3413, 0.3044, 0.3367, 0.3129, 0.2227, 0.3676, 0.2292, 0.3608, 0.3431, 0.2768, 0.2307, 0.3122, 0.3567, 0.2388, 0.3597, 0.245, 0.3514, 0.3517, 0.2203, 0.2273, 0.3294, 0.3614, 0.2537, 0.3213, 0.3074, 0.2321, 0.2318, 0.3539, 0.2281, 0.2386, 0.3472, 0.2772, 0.3, 0.3, 0.3459, 0.3069, 0.3325, 0.3334, 0.3, 0.3233, 0.3477, 0.2251, 0.3568, 0.3309, 0.3517, 0.3586, 0.2332, 0.2338, 0.3108, 0.2954, 0.3397, 0.3515, 0.2262, 0.2235, 0.2861, 0.3105, 0.2256, 0.2208, 0.3313, 0.2252, 0.2289, 0.3528, 0.3225, 0.378, 0.3, 0.3712, 0.3422, 0.2596, 0.2589, 0.3612, 0.3545, 0.327, 0.3056, 0.3651, 0.3176, 0.2222, 0.3, 0.2275, 0.2586, 0.3116, 0.344, 0.2581, 0.2276, 0.3427, 0.3086, 0.3509, 0.2279, 0.2181, 0.2265, 0.344, 0.2202, 0.3356, 0.2854, 0.2245, 0.3426, 0.2263, 0.3681, 0.228, 0.3488, 0.3346, 0.3, 0.3578, 0.2218, 0.3533, 0.3, 0.2222, 0.2224, 0.2792, 0.3103, 0.2195, 0.2221, 0.3295, 0.3454, 0.3, 0.2831, 0.3527, 0.3, 0.2272, 0.3502, 0.3137, 0.3075, 0.3, 0.3403, 0.2258, 0.3143, 0.3079, 0.2263, 0.3, 0.3674, 0.3, 0.3739, 0.3, 0.3059, 0.3, 0.2723, 0.351, 0.2456, 0.3492, 0.2929, 0.3573, 0.2243, 0.3, 0.3496, 0.227, 0.322, 0.3445, 0.3509, 0.3337, 0.2256, 0.2281, 0.3532, 0.3376, 0.2722, 0.2294, 0.3, 0.2412, 0.2426, 0.3167, 0.3274, 0.2216, 0.2235, 0.2292, 0.2235, 0.3, 0.3, 0.2849, 0.3373, 0.3393, 0.2259, 0.2448, 0.2836, 0.3465, 0.2949, 0.2259, 0.3, 0.2866, 0.3, 0.3336, 0.275, 0.2665, 0.2301, 0.2661, 0.3612, 0.2827, 0.3481, 0.3588, 0.3378, 0.315, 0.343, 0.2423, 0.3, 0.3732, 0.2308, 0.2226, 0.2421, 0.2302, 0.3666, 0.3, 0.326, 0.2674, 0.3651, 0.3, 0.2767, 0.3007, 0.2889, 0.228, 0.3737, 0.238, 0.3707, 0.3, 0.2291, 0.346, 0.3671, 0.3, 0.2245, 0.3351, 0.2527, 0.2569, 0.3064, 0.327, 0.2488, 0.3396, 0.3, 0.3014, 0.3511, 0.2321, 0.3, 0.3128, 0.3, 0.3, 0.2321, 0.2491, 0.2683, 0.2927, 0.3702, 0.3, 0.2311, 0.3603, 0.2707, 0.3237, 0.3, 0.3286, 0.306, 0.2284, 0.3686, 0.3564, 0.3, 0.2362, 0.3384, 0.3008, 0.2206, 0.361, 0.3723, 0.2673, 0.2913, 0.3377], score: 0.5839\n",
      "    Epoch: 8, Time: 14.3694 s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-084e31e37340>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clipwise_output'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbird_smooth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mopt_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold_params\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyper_params_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_optimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthreshold_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/autoth/core.py\u001b[0m in \u001b[0;36mdo_optimize\u001b[0;34m(self, init_params)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0me\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetNewParams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/autoth/core.py\u001b[0m in \u001b[0;36mcalculate_gradients\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mcnt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mnew_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mnew_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_calculator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_score\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-be10dc017e0f>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mthresholds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m                     \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from IPython.display import display\n",
    "from utils.logging import Averager\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "epochs=15\n",
    "learning_rate = 0.1\n",
    "model = Tmodel().to(torch.float32)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, \n",
    "        betas=(0.9, 0.999), eps=1e-08, weight_decay=0., amsgrad=True)\n",
    "\n",
    "score_calculator = ScoreCalculatorExample(dataloader.batch_size, classes_num)\n",
    "hyper_params_opt = HyperParamsOptimizer(score_calculator,learning_rate=1e-2, epochs=10)\n",
    "\n",
    "threshold_params = [0.3]*classes_num\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "avg = Averager()\n",
    "scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, steps_per_epoch=len(dataloader), epochs=epochs,\n",
    "                                        pct_start=0.2)\n",
    "dh = display('',display_id=True)\n",
    "batch_max = len(dataloader)\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    #train\n",
    "    for batch, data in enumerate(dataloader):\n",
    "        wav, bird = data\n",
    "        \n",
    "        wav = torch.from_numpy(wav).to(torch.float32)\n",
    "        wav = wav.to(device)\n",
    "        \n",
    "        bird_smooth = np.where(bird==1,0.995,0.0025)\n",
    "        bird_smooth = torch.from_numpy(bird_smooth).to(torch.float32).to(device)\n",
    "        output_dict = model(wav)\n",
    "        \n",
    "        loss = F.binary_cross_entropy(output_dict['clipwise_output'], bird_smooth)\n",
    "        (opt_score, threshold_params) = hyper_params_opt.do_optimize(init_params=threshold_params)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        scheduler.step()\n",
    "        avg.add(loss)\n",
    "        dh.update('Epoch : {} {}/{} loss : {} / lr : {} / f1 : {}'.format(\\\n",
    "                                epoch+1,batch+1,batch_max,avg.val(),\\\n",
    "                                optimizer.param_groups[0]['lr'],opt_score))\n",
    "        \n",
    "        \n",
    "        del wav, bird, loss, output_dict, data\n",
    "        torch.save(model.state_dict(),os.path.join('./result/sed_v1.pth'))\n",
    "    #eval\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "saved_model = './result/sed.pth'\n",
    "val_path = './asset/birdclef-2021/train_soundscapes/'\n",
    "val_list = [i for i in os.listdir(val_path) if i.split('.')[-1]=='ogg']\n",
    "val_df = pd.read_csv('./asset/birdclef-2021/train_soundscape_labels.csv')\n",
    "\n",
    "classes_list = np.array(blist)\n",
    "\n",
    "model = Tmodel().to(torch.float32)\n",
    "model.load_state_dict(torch.load(saved_model,map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "model.eval()\n",
    "for sound_scape in val_list:\n",
    "    sr = 32000\n",
    "    duration = 5 * sr\n",
    "    scape, _ = librosa.load(os.path.join(val_path,sound_scape),sr=sr)\n",
    "    \n",
    "    for i in range(0,len(scape),duration):\n",
    "        temp = scape[i:i+duration]\n",
    "        temp = torch.from_numpy(temp[np.newaxis,:]).to(device)\n",
    "        output_dict = model(temp)\n",
    "        \n",
    "        output_pred = output_dict['clipwise_output'].cpu().detach().numpy()\n",
    "        pred = classes_list[output_pred[0]>0.03]\n",
    "        print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.72674030e-05, 8.32735386e-05, 8.73663303e-05, 5.93038021e-05,\n",
       "       1.02584294e-04, 7.76490560e-05, 2.10788159e-04, 5.54342005e-05,\n",
       "       1.05157815e-04, 1.11761707e-04, 2.49704346e-04, 5.45745534e-05,\n",
       "       2.13419509e-04, 9.79089527e-05, 8.89313669e-05, 1.04112936e-04,\n",
       "       1.29907610e-04, 6.13467928e-05, 6.70758454e-05, 3.68852052e-05,\n",
       "       1.49424421e-04, 1.08961693e-04, 2.49427481e-04, 4.65938756e-05,\n",
       "       8.22638976e-05, 4.81108145e-05, 2.41485843e-03, 8.82633904e-05,\n",
       "       5.50324839e-05, 2.03678996e-04, 1.00064251e-04, 2.66559975e-04,\n",
       "       8.47917836e-05, 7.79983748e-05, 5.03541669e-04, 7.74042564e-05,\n",
       "       4.54095192e-04, 1.44496188e-03, 1.50909938e-04, 1.27710402e-04,\n",
       "       6.29137794e-05, 7.94669031e-05, 5.79749139e-05, 1.16624054e-03,\n",
       "       7.22384197e-04, 1.48095249e-04, 6.66684718e-05, 3.20672698e-04,\n",
       "       1.21206707e-04, 1.37347961e-04, 1.18023483e-04, 1.48119288e-04,\n",
       "       1.65133300e-04, 1.08969602e-04, 4.64126177e-04, 2.58609041e-04,\n",
       "       1.21356898e-04, 1.83501092e-04, 1.92408508e-04, 7.46238584e-05,\n",
       "       3.79630132e-04, 2.14750035e-04, 4.74333094e-04, 3.11202544e-04,\n",
       "       1.16851144e-04, 8.99171355e-05, 9.35143762e-05, 1.16123774e-04,\n",
       "       2.04309807e-04, 5.81830718e-05, 8.18170956e-05, 2.69093318e-04,\n",
       "       7.87297176e-05, 9.25762724e-05, 1.54577530e-04, 6.00208179e-04,\n",
       "       1.02047416e-04, 1.59042713e-04, 1.45251746e-04, 1.89246726e-04,\n",
       "       2.01657589e-04, 1.63117656e-04, 9.87582753e-05, 3.79370002e-04,\n",
       "       2.00504641e-04, 7.33075140e-05, 1.39416399e-04, 2.01055867e-04,\n",
       "       1.04233521e-04, 1.02822305e-04, 2.56072701e-04, 7.06632200e-05,\n",
       "       5.85608395e-05, 2.99320964e-04, 9.41179387e-05, 6.04875895e-05,\n",
       "       8.44684400e-05, 1.11538742e-04, 1.46830134e-04, 4.35501774e-04,\n",
       "       9.11035750e-05, 1.01781443e-04, 1.01507758e-04, 8.56199622e-05,\n",
       "       2.45182542e-04, 1.08169683e-04, 1.06740357e-04, 7.44713907e-05,\n",
       "       1.72236847e-04, 2.34373438e-04, 1.14286202e-04, 8.68656643e-05,\n",
       "       1.14677263e-04, 5.24019015e-05, 1.09461565e-04, 6.03633188e-03,\n",
       "       7.00836390e-05, 1.19054312e-04, 9.56334407e-05, 7.85236844e-05,\n",
       "       4.90145176e-05, 1.34172995e-04, 1.97951726e-04, 2.70788500e-04,\n",
       "       1.33543363e-04, 1.97136353e-04, 3.08270042e-04, 1.09304485e-04,\n",
       "       5.28576202e-05, 1.72692497e-04, 1.47449347e-04, 8.93562319e-05,\n",
       "       1.28174099e-04, 9.91494016e-05, 2.04542681e-04, 5.72736753e-05,\n",
       "       4.60573829e-05, 1.06772975e-04, 6.91030873e-05, 1.44992955e-04,\n",
       "       1.18310105e-04, 4.03115409e-05, 7.37656228e-05, 1.06107800e-04,\n",
       "       6.85873165e-05, 1.04624865e-04, 1.38961957e-04, 6.46896474e-03,\n",
       "       3.50793620e-04, 1.12226384e-03, 1.01415724e-04, 6.06913236e-05,\n",
       "       2.33803425e-04, 1.42292818e-04, 3.22081978e-05, 1.60449068e-04,\n",
       "       1.23826147e-04, 3.29956703e-04, 2.14683547e-04, 3.02915461e-04,\n",
       "       5.39304383e-05, 1.51082320e-04, 3.00849468e-04, 4.36575610e-05,\n",
       "       1.89431070e-04, 8.30132194e-05, 7.20322860e-05, 1.39273499e-04,\n",
       "       4.46624414e-04, 6.02077125e-05, 1.26466955e-04, 1.05378065e-04,\n",
       "       1.15786701e-04, 1.38767253e-04, 1.78940958e-04, 1.16398704e-04,\n",
       "       2.49929028e-04, 1.06817941e-04, 2.22294664e-04, 1.54288078e-04,\n",
       "       1.43163139e-04, 1.61762291e-04, 1.33249778e-04, 4.97120891e-05,\n",
       "       7.64782308e-05, 1.62402212e-04, 1.85509663e-04, 8.07848264e-05,\n",
       "       2.34215840e-04, 2.72682984e-04, 4.68037586e-04, 1.82602758e-04,\n",
       "       2.05420889e-04, 1.91861822e-04, 4.47359722e-04, 1.15852068e-04,\n",
       "       1.96205205e-04, 1.11362126e-04, 9.34003692e-05, 1.10594032e-04,\n",
       "       3.06911941e-04, 2.68122327e-04, 1.49162559e-04, 1.25419960e-04,\n",
       "       9.35303833e-05, 1.11122950e-04, 2.24692893e-04, 1.01942860e-04,\n",
       "       2.37832792e-04, 3.81990176e-05, 1.03948354e-04, 9.59983590e-05,\n",
       "       6.09965391e-05, 7.34289788e-05, 1.39792042e-04, 1.40082193e-04,\n",
       "       1.86397985e-04, 5.85475937e-04, 6.00121915e-04, 3.83277482e-04,\n",
       "       1.40079908e-04, 1.34524089e-04, 4.82227624e-05, 2.28990117e-04,\n",
       "       1.37392592e-04, 6.64468098e-05, 2.00482187e-04, 5.57486317e-04,\n",
       "       1.03617640e-04, 1.77662878e-04, 1.52625231e-04, 9.99000622e-05,\n",
       "       7.41010590e-05, 8.24697345e-05, 7.13418922e-05, 9.16925928e-05,\n",
       "       2.94052967e-04, 7.43957717e-05, 1.77693320e-04, 7.05703933e-05,\n",
       "       3.88075423e-04, 8.01955815e-04, 7.02752586e-05, 1.28586616e-04,\n",
       "       7.80146001e-05, 1.65968435e-04, 4.08166670e-05, 1.94442880e-04,\n",
       "       1.84193428e-04, 8.88756331e-05, 1.57705421e-04, 1.42443532e-04,\n",
       "       7.13903428e-05, 1.50374282e-04, 1.01347840e-04, 9.70957917e-05,\n",
       "       9.65336803e-05, 9.84005455e-05, 2.18650515e-04, 1.39955169e-04,\n",
       "       2.75433529e-04, 2.41995964e-04, 1.12959970e-04, 1.33892318e-04,\n",
       "       3.66285822e-05, 1.02919228e-04, 5.71281198e-05, 8.25466850e-05,\n",
       "       5.01738432e-05, 6.07008405e-04, 2.98053346e-04, 8.48289055e-05,\n",
       "       1.35363778e-04, 1.67346618e-04, 1.21961988e-04, 9.46699991e-04,\n",
       "       8.88785507e-05, 1.25555802e-04, 1.03601262e-04, 1.23500577e-04,\n",
       "       4.13077913e-04, 3.68473469e-04, 1.37073599e-04, 1.47888888e-04,\n",
       "       1.01133606e-04, 1.37982599e-04, 8.19494308e-05, 6.22376319e-05,\n",
       "       5.05207936e-05, 4.98465706e-05, 1.03034050e-04, 7.82242205e-05,\n",
       "       2.52998841e-04, 2.34656545e-04, 6.15602548e-05, 8.65921756e-05,\n",
       "       3.67922694e-05, 1.34448332e-04, 1.80633774e-03, 1.15577743e-04,\n",
       "       1.01593701e-04, 6.22728694e-05, 6.61943413e-05, 5.86691203e-05,\n",
       "       8.45021859e-05, 1.14133931e-04, 1.66712765e-04, 5.72062236e-05,\n",
       "       1.54731155e-04, 1.20869256e-04, 1.16175528e-04, 7.38823845e-04,\n",
       "       2.27995588e-05, 5.15154679e-04, 8.53283418e-05, 9.16154822e-04,\n",
       "       1.31586479e-04, 8.34684251e-05, 1.41749450e-04, 1.84422373e-04,\n",
       "       2.27073004e-04, 8.89993098e-05, 8.95880803e-05, 2.01963529e-04,\n",
       "       4.63874225e-04, 7.68287864e-05, 1.11464644e-04, 1.81030409e-04,\n",
       "       8.58520070e-05, 1.95176210e-04, 1.24519793e-04, 1.40084812e-04,\n",
       "       2.62984540e-04, 3.76966927e-05, 1.21647798e-04, 1.07902131e-04,\n",
       "       1.34394213e-04, 1.47473198e-04, 5.75468061e-04, 1.00768229e-04,\n",
       "       1.46170758e-04, 1.02237187e-04, 1.02635648e-04, 8.95669073e-05,\n",
       "       1.32526824e-04, 1.23386795e-04, 7.51814805e-05, 1.52685330e-04,\n",
       "       1.24581129e-04, 6.86057392e-05, 1.05858315e-04, 3.59472993e-04,\n",
       "       5.07667777e-04, 3.50357790e-04, 7.32355402e-05, 7.61764895e-05,\n",
       "       1.29918568e-04, 2.01758172e-04, 1.05443527e-04, 2.28140547e-04,\n",
       "       7.44075223e-05, 6.52815797e-05, 2.14106301e-04, 6.41344086e-05,\n",
       "       9.27550063e-05, 1.32387431e-04, 3.46773129e-04, 5.24276038e-05,\n",
       "       6.75351330e-05, 2.16615270e-04, 5.87497598e-05, 1.86772435e-04,\n",
       "       3.56621676e-05, 7.33554989e-05, 1.20875593e-04, 5.88547773e-05,\n",
       "       4.24824320e-05, 5.34021601e-05, 4.21573583e-04, 7.47097802e-05,\n",
       "       1.49280560e-04, 8.13537772e-05, 8.75168043e-05, 1.14865026e-04,\n",
       "       4.28923231e-05, 6.21541840e-05, 4.80691582e-04, 8.05615346e-05,\n",
       "       1.01377831e-04, 1.83793134e-04, 3.53250580e-05, 3.54789954e-04,\n",
       "       8.54510727e-05, 5.52668680e-05, 5.91670432e-05, 1.47135870e-04,\n",
       "       9.48825036e-05, 2.04639204e-04], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tmodel(\n",
      "  (spectrogram_extractor): Spectrogram(\n",
      "    (stft): STFT(\n",
      "      (conv_real): Conv1d(1, 1025, kernel_size=(2048,), stride=(627,), bias=False)\n",
      "      (conv_imag): Conv1d(1, 1025, kernel_size=(2048,), stride=(627,), bias=False)\n",
      "    )\n",
      "  )\n",
      "  (logmel_extractor): LogmelFilterBank()\n",
      "  (spec_augmenter): SpecAugmentation(\n",
      "    (time_dropper): DropStripes()\n",
      "    (freq_dropper): DropStripes()\n",
      "  )\n",
      "  (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_block1): ConvBlock(\n",
      "    (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv_block2): ConvBlock(\n",
      "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv_block3): ConvBlock(\n",
      "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv_block4): ConvBlock(\n",
      "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (gru): GRU(512, 256, batch_first=True, bidirectional=True)\n",
      "  (att_block): AttBlock(\n",
      "    (att): Conv1d(512, 398, kernel_size=(1,), stride=(1,))\n",
      "    (cla): Conv1d(512, 398, kernel_size=(1,), stride=(1,))\n",
      "    (bn_att): BatchNorm1d(398, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 2 inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init_layer(layer):\n",
    "    \"\"\"Initialize a Linear or Convolutional layer. \"\"\"\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    " \n",
    "    if hasattr(layer, 'bias'):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.)\n",
    "            \n",
    "    \n",
    "def init_bn(bn):\n",
    "    \"\"\"Initialize a Batchnorm layer. \"\"\"\n",
    "    bn.bias.data.fill_(0.)\n",
    "    bn.weight.data.fill_(1.)\n",
    "\n",
    "\n",
    "def init_gru(rnn):\n",
    "    \"\"\"Initialize a GRU layer. \"\"\"\n",
    "    \n",
    "    def _concat_init(tensor, init_funcs):\n",
    "        (length, fan_out) = tensor.shape\n",
    "        fan_in = length // len(init_funcs)\n",
    "    \n",
    "        for (i, init_func) in enumerate(init_funcs):\n",
    "            init_func(tensor[i * fan_in : (i + 1) * fan_in, :])\n",
    "        \n",
    "    def _inner_uniform(tensor):\n",
    "        fan_in = nn.init._calculate_correct_fan(tensor, 'fan_in')\n",
    "        nn.init.uniform_(tensor, -math.sqrt(3 / fan_in), math.sqrt(3 / fan_in))\n",
    "    \n",
    "    for i in range(rnn.num_layers):\n",
    "        _concat_init(\n",
    "            getattr(rnn, 'weight_ih_l{}'.format(i)),\n",
    "            [_inner_uniform, _inner_uniform, _inner_uniform]\n",
    "        )\n",
    "        torch.nn.init.constant_(getattr(rnn, 'bias_ih_l{}'.format(i)), 0)\n",
    "\n",
    "        _concat_init(\n",
    "            getattr(rnn, 'weight_hh_l{}'.format(i)),\n",
    "            [_inner_uniform, _inner_uniform, nn.init.orthogonal_]\n",
    "        )\n",
    "        torch.nn.init.constant_(getattr(rnn, 'bias_hh_l{}'.format(i)), 0)\n",
    "        \n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \n",
    "        super(ConvBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, \n",
    "                              out_channels=out_channels,\n",
    "                              kernel_size=(3, 3), stride=(1, 1),\n",
    "                              padding=(1, 1), bias=False)\n",
    "                              \n",
    "        self.conv2 = nn.Conv2d(in_channels=out_channels, \n",
    "                              out_channels=out_channels,\n",
    "                              kernel_size=(3, 3), stride=(1, 1),\n",
    "                              padding=(1, 1), bias=False)\n",
    "                              \n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.init_weight()\n",
    "        \n",
    "    def init_weight(self):\n",
    "        init_layer(self.conv1)\n",
    "        init_layer(self.conv2)\n",
    "        init_bn(self.bn1)\n",
    "        init_bn(self.bn2)\n",
    "\n",
    "        \n",
    "    def forward(self, input, pool_size=(2, 2), pool_type='avg'):\n",
    "        \n",
    "        x = input\n",
    "        x = F.relu_(self.bn1(self.conv1(x)))\n",
    "        x = F.relu_(self.bn2(self.conv2(x)))\n",
    "        if pool_type == 'max':\n",
    "            x = F.max_pool2d(x, kernel_size=pool_size)\n",
    "        elif pool_type == 'avg':\n",
    "            x = F.avg_pool2d(x, kernel_size=pool_size)\n",
    "        elif pool_type == 'avg+max':\n",
    "            x1 = F.avg_pool2d(x, kernel_size=pool_size)\n",
    "            x2 = F.max_pool2d(x, kernel_size=pool_size)\n",
    "            x = x1 + x2\n",
    "        else:\n",
    "            raise Exception('Incorrect argument!')\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "class AttBlockV2(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 out_features: int,\n",
    "                 activation=\"linear\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.att = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "        self.cla = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.att)\n",
    "        init_layer(self.cla)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (n_samples, n_in, n_time)\n",
    "        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        return x, norm_att, cla\n",
    "\n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == 'linear':\n",
    "            return x\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return torch.sigmoid(x)\n",
    "\n",
    "    \n",
    "\n",
    "class Tmodel(nn.Module):\n",
    "    def __init__(self,train=True):\n",
    "        super(Tmodel,self).__init__()\n",
    "        \n",
    "        SPEC_HEIGHT = 128\n",
    "        SPEC_WIDTH = 256\n",
    "        NUM_MELS = SPEC_HEIGHT\n",
    "        HOP_LENGTH = int(32000 * 5 / (SPEC_WIDTH - 1)) # sample rate * duration / spec width - 1 == 627\n",
    "        FMIN = 500\n",
    "        FMAX = 12500\n",
    "        classes_num = 398\n",
    "        self.interpolate_ratio = 8\n",
    "        \n",
    "        self.spectrogram_extractor  = Spectrogram(\n",
    "                    n_fft=2048,\n",
    "                    hop_length=HOP_LENGTH,\n",
    "                    freeze_parameters=True)\n",
    "        \n",
    "        self.logmel_extractor = LogmelFilterBank(sr=32000,\n",
    "            n_mels=NUM_MELS, fmin=FMIN, fmax=FMAX, freeze_parameters=True)\n",
    "\n",
    "        self.spec_augmenter = SpecAugmentation(time_drop_width=64, time_stripes_num=2, \n",
    "            freq_drop_width=8, freq_stripes_num=2)\n",
    "        \n",
    "        self.bn0 = nn.BatchNorm2d(128)\n",
    "\n",
    "        # load pretrained models, using ResNeSt-50 as an example\n",
    "        if train:\n",
    "            base_model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest50', pretrained=True)\n",
    "        else:\n",
    "            base_model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest50', pretrained=False)\n",
    "            \n",
    "        layers = list(base_model.children())[:-2]\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        \n",
    "        self.gru = nn.GRU(input_size=2048, hidden_size=1024, num_layers=1, \n",
    "            bias=True, batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.att_block = AttBlockV2(2048, classes_num, activation='sigmoid')\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        init_bn(self.bn0)\n",
    "        init_gru(self.gru)\n",
    "        \n",
    "    def forward(self,input,mixup_lambda=None):\n",
    "        \n",
    "        x = self.spectrogram_extractor(input)   # (batch_size, 1, time_steps, freq_bins)\n",
    "        x = self.logmel_extractor(x)    # (batch_size, 1, time_steps, mel_bins)\n",
    "        \n",
    "        frames_num = x.shape[2]\n",
    "        \n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "        \n",
    "        if self.training:\n",
    "            x = self.spec_augmenter(x)\n",
    "        \n",
    "        # Mixup on spectrogram\n",
    "        if self.training and mixup_lambda is not None:\n",
    "            x = do_mixup(x, mixup_lambda)\n",
    "        \n",
    "        x = torch.tile(x,(1,3,1,1))\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        x = torch.mean(x, dim=3)\n",
    "        x = x.transpose(1, 2)   # (batch_size, time_steps, channels)\n",
    "        (x, _) = self.gru(x)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        (clipwise_output, norm_att, segmentwise_output) = self.att_block(x)\n",
    "        \"\"\"cla: (batch_size, classes_num, time_stpes)\"\"\"\n",
    "        \n",
    "        logit = torch.sum(norm_att * self.att_block.cla(x), dim=2)\n",
    "        segmentwise_logit = self.att_block.cla(x).transpose(1, 2)\n",
    "        segmentwise_output = segmentwise_output.transpose(1, 2)\n",
    "\n",
    "        interpolate_ratio = frames_num // segmentwise_output.size(1)\n",
    "        \n",
    "        # Framewise output\n",
    "        framewise_output = interpolate(segmentwise_output,\n",
    "                                       interpolate_ratio)\n",
    "        framewise_output = pad_framewise_output(framewise_output, frames_num)\n",
    "\n",
    "        framewise_logit = interpolate(segmentwise_logit, interpolate_ratio)\n",
    "        framewise_logit = pad_framewise_output(framewise_logit, frames_num)\n",
    "        \n",
    "        output_dict = {\n",
    "            \"framewise_output\": framewise_output,\n",
    "            \"segmentwise_output\": segmentwise_output,\n",
    "            \"logit\": logit,\n",
    "            \"framewise_logit\": framewise_logit,\n",
    "            \"clipwise_output\": clipwise_output\n",
    "        }\n",
    "\n",
    "            \n",
    "        return output_dict\n",
    "    \n",
    "def interpolate(x: torch.Tensor, ratio: int):\n",
    "    \"\"\"Interpolate data in time domain. This is used to compensate the\n",
    "    resolution reduction in downsampling of a CNN.\n",
    "    Args:\n",
    "      x: (batch_size, time_steps, classes_num)\n",
    "      ratio: int, ratio to interpolate\n",
    "    Returns:\n",
    "      upsampled: (batch_size, time_steps * ratio, classes_num)\n",
    "    \"\"\"\n",
    "    (batch_size, time_steps, classes_num) = x.shape\n",
    "    upsampled = x[:, :, None, :].repeat(1, 1, ratio, 1)\n",
    "    upsampled = upsampled.reshape(batch_size, time_steps * ratio, classes_num)\n",
    "    return upsampled\n",
    "\n",
    "\n",
    "def pad_framewise_output(framewise_output: torch.Tensor, frames_num: int):\n",
    "    \"\"\"Pad framewise_output to the same length as input frames. The pad value\n",
    "    is the same as the value of the last frame.\n",
    "    Args:\n",
    "      framewise_output: (batch_size, frames_num, classes_num)\n",
    "      frames_num: int, number of frames to pad\n",
    "    Outputs:\n",
    "      output: (batch_size, frames_num, classes_num)\n",
    "    \"\"\"\n",
    "    output = F.interpolate(\n",
    "        framewise_output.unsqueeze(1),\n",
    "        size=(frames_num, framewise_output.size(2)),\n",
    "        align_corners=True,\n",
    "        mode=\"bilinear\").squeeze(1)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/zhanghang1989_ResNeSt_master\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-995f42de4fa1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         betas=(0.9, 0.999), eps=1e-08, weight_decay=0., amsgrad=True)\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    663\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 665\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    393\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    393\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    393\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    415\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    661\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    662\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 663\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#train\n",
    "import torch\n",
    "from IPython.display import display\n",
    "from utils.logging import Averager\n",
    "from torch.optim import lr_scheduler\n",
    "from utils.loss import AsymmetricLoss\n",
    "\n",
    "#learning_rate = 0.1 #for onecycle\n",
    "learning_rate = 0.01  #for cosine\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "epochs=15\n",
    "model = Tmodel().to(torch.float32)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, \n",
    "        betas=(0.9, 0.999), eps=1e-08, weight_decay=0., amsgrad=True)\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "avg = Averager()\n",
    "#scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, steps_per_epoch=len(dataloader), epochs=epochs,\n",
    "#                                        pct_start=0.2)\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "dh = display('',display_id=True)\n",
    "batch_max = len(dataloader)\n",
    "loss_func = BCEFocal2WayLoss()\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    #train\n",
    "    for batch, data in enumerate(dataloader):\n",
    "        wav, bird = data\n",
    "        \n",
    "        wav = torch.from_numpy(wav).to(torch.float32)\n",
    "        wav = wav.to(device)\n",
    "        \n",
    "        bird_smooth = np.where(bird==1,0.995,0.0025)\n",
    "        bird_smooth = torch.from_numpy(bird_smooth).to(torch.float32).to(device)\n",
    "        output_dict = model(wav)\n",
    "        \n",
    "        loss = loss_func(output_dict, bird_smooth)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        scheduler.step()\n",
    "        avg.add(loss)\n",
    "        dh.update('Epoch : {} {}/{} loss : {} / lr : {}'.format(\\\n",
    "                                epoch+1,batch+1,batch_max,avg.val(),\\\n",
    "                                optimizer.param_groups[0]['lr']))\n",
    "        \n",
    "        del wav, bird, loss, output_dict, data\n",
    "        torch.save(model.state_dict(),os.path.join('./result/sed_asloss.pth'))\n",
    "    #eval\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/zhanghang1989_ResNeSt_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2782_SSW_20170701.ogg []\n",
      "2782_SSW_20170701.ogg []\n",
      "2782_SSW_20170701.ogg []\n",
      "2782_SSW_20170701.ogg ['rucwar']\n",
      "2782_SSW_20170701.ogg []\n",
      "2782_SSW_20170701.ogg ['brbmot1' 'goowoo1' 'comgol' 'gockin']\n",
      "2782_SSW_20170701.ogg []\n",
      "2782_SSW_20170701.ogg ['brbmot1']\n",
      "2782_SSW_20170701.ogg ['brbmot1' 'goowoo1' 'comgol' 'trogna1']\n",
      "2782_SSW_20170701.ogg ['brbmot1']\n",
      "2782_SSW_20170701.ogg ['brbmot1' 'goowoo1']\n",
      "2782_SSW_20170701.ogg ['brbmot1' 'goowoo1']\n",
      "2782_SSW_20170701.ogg ['brbmot1' 'goowoo1']\n",
      "2782_SSW_20170701.ogg ['brbmot1']\n",
      "2782_SSW_20170701.ogg []\n",
      "2782_SSW_20170701.ogg []\n",
      "2782_SSW_20170701.ogg ['rucwar']\n",
      "2782_SSW_20170701.ogg ['norfli' 'brbmot1']\n",
      "2782_SSW_20170701.ogg ['brbmot1']\n",
      "2782_SSW_20170701.ogg ['brbmot1' 'sander']\n",
      "2782_SSW_20170701.ogg ['trogna1']\n",
      "2782_SSW_20170701.ogg ['brbmot1' 'goowoo1' 'comgol' 'trogna1']\n",
      "2782_SSW_20170701.ogg ['brbmot1' 'trogna1']\n",
      "2782_SSW_20170701.ogg ['brbmot1' 'sander']\n",
      "2782_SSW_20170701.ogg ['brbmot1' 'goowoo1' 'comgol']\n",
      "2782_SSW_20170701.ogg []\n",
      "2782_SSW_20170701.ogg []\n",
      "2782_SSW_20170701.ogg ['brbmot1']\n",
      "2782_SSW_20170701.ogg ['norfli' 'brbmot1' 'goowoo1' 'comgol' 'trogna1']\n",
      "2782_SSW_20170701.ogg ['brbmot1']\n",
      "2782_SSW_20170701.ogg ['brbmot1']\n",
      "2782_SSW_20170701.ogg ['brbmot1' 'trogna1']\n",
      "2782_SSW_20170701.ogg ['brbmot1']\n",
      "2782_SSW_20170701.ogg []\n",
      "2782_SSW_20170701.ogg []\n",
      "2782_SSW_20170701.ogg ['brbmot1' 'goowoo1']\n",
      "2782_SSW_20170701.ogg ['brbmot1' 'goowoo1']\n",
      "2782_SSW_20170701.ogg ['brbmot1' 'trogna1']\n",
      "2782_SSW_20170701.ogg []\n",
      "2782_SSW_20170701.ogg ['brbmot1']\n",
      "2782_SSW_20170701.ogg []\n",
      "2782_SSW_20170701.ogg []\n",
      "2782_SSW_20170701.ogg ['lotman1']\n",
      "2782_SSW_20170701.ogg ['norfli' 'brbmot1' 'trogna1']\n",
      "2782_SSW_20170701.ogg []\n",
      "2782_SSW_20170701.ogg ['brbmot1']\n",
      "2782_SSW_20170701.ogg ['brbmot1']\n",
      "2782_SSW_20170701.ogg ['brbmot1' 'trogna1']\n",
      "2782_SSW_20170701.ogg ['brbmot1']\n",
      "2782_SSW_20170701.ogg []\n",
      "2782_SSW_20170701.ogg []\n",
      "2782_SSW_20170701.ogg ['brbmot1']\n",
      "2782_SSW_20170701.ogg ['brbmot1']\n",
      "2782_SSW_20170701.ogg ['norfli' 'brbmot1' 'comgol' 'trogna1']\n",
      "2782_SSW_20170701.ogg ['brbmot1']\n",
      "2782_SSW_20170701.ogg ['norfli' 'brbmot1' 'comgol' 'trogna1']\n",
      "2782_SSW_20170701.ogg ['norfli' 'brbmot1' 'comgol']\n",
      "2782_SSW_20170701.ogg ['brbmot1' 'trogna1']\n",
      "2782_SSW_20170701.ogg ['norfli' 'brbmot1']\n",
      "2782_SSW_20170701.ogg ['trogna1']\n",
      "2782_SSW_20170701.ogg ['trogna1']\n",
      "2782_SSW_20170701.ogg ['goowoo1']\n",
      "2782_SSW_20170701.ogg ['norfli' 'brbmot1' 'trogna1']\n",
      "2782_SSW_20170701.ogg ['norfli' 'goowoo1']\n",
      "2782_SSW_20170701.ogg ['norfli' 'brbmot1' 'comgol' 'trogna1']\n",
      "2782_SSW_20170701.ogg ['brbmot1']\n",
      "2782_SSW_20170701.ogg ['norfli' 'brbmot1' 'trogna1']\n",
      "2782_SSW_20170701.ogg ['brbmot1']\n",
      "2782_SSW_20170701.ogg ['brbmot1' 'trogna1']\n",
      "2782_SSW_20170701.ogg ['brbmot1']\n",
      "2782_SSW_20170701.ogg ['brbmot1']\n",
      "2782_SSW_20170701.ogg ['brbmot1']\n",
      "2782_SSW_20170701.ogg ['brbmot1']\n",
      "2782_SSW_20170701.ogg []\n",
      "2782_SSW_20170701.ogg []\n",
      "2782_SSW_20170701.ogg ['brbmot1']\n",
      "2782_SSW_20170701.ogg ['norfli' 'brbmot1' 'trogna1']\n",
      "2782_SSW_20170701.ogg []\n",
      "2782_SSW_20170701.ogg ['brbmot1']\n",
      "2782_SSW_20170701.ogg ['norfli' 'brbmot1']\n",
      "2782_SSW_20170701.ogg ['brbmot1']\n",
      "2782_SSW_20170701.ogg ['brbmot1']\n",
      "2782_SSW_20170701.ogg ['brbmot1']\n",
      "2782_SSW_20170701.ogg ['brbmot1']\n",
      "2782_SSW_20170701.ogg ['brbmot1']\n",
      "2782_SSW_20170701.ogg ['brbmot1']\n",
      "2782_SSW_20170701.ogg ['brbmot1']\n",
      "2782_SSW_20170701.ogg []\n",
      "2782_SSW_20170701.ogg ['amegfi']\n",
      "2782_SSW_20170701.ogg ['brbmot1']\n",
      "2782_SSW_20170701.ogg ['brbmot1']\n",
      "2782_SSW_20170701.ogg ['brbmot1']\n",
      "2782_SSW_20170701.ogg ['brbmot1']\n",
      "2782_SSW_20170701.ogg []\n",
      "2782_SSW_20170701.ogg []\n",
      "2782_SSW_20170701.ogg []\n",
      "2782_SSW_20170701.ogg ['brbmot1']\n",
      "2782_SSW_20170701.ogg ['norfli' 'brbmot1']\n",
      "2782_SSW_20170701.ogg ['brbmot1']\n",
      "2782_SSW_20170701.ogg ['brbmot1']\n",
      "2782_SSW_20170701.ogg ['brbmot1' 'goowoo1' 'lotman1']\n",
      "2782_SSW_20170701.ogg ['trogna1']\n",
      "2782_SSW_20170701.ogg []\n",
      "2782_SSW_20170701.ogg []\n",
      "2782_SSW_20170701.ogg ['brbmot1']\n",
      "2782_SSW_20170701.ogg []\n",
      "2782_SSW_20170701.ogg []\n",
      "2782_SSW_20170701.ogg ['norfli' 'brbmot1' 'comgol']\n",
      "2782_SSW_20170701.ogg []\n",
      "2782_SSW_20170701.ogg []\n",
      "2782_SSW_20170701.ogg ['norfli' 'brbmot1']\n",
      "2782_SSW_20170701.ogg ['norfli']\n",
      "2782_SSW_20170701.ogg ['sposan']\n",
      "2782_SSW_20170701.ogg []\n",
      "2782_SSW_20170701.ogg ['trogna1']\n",
      "2782_SSW_20170701.ogg ['norfli']\n",
      "2782_SSW_20170701.ogg []\n",
      "2782_SSW_20170701.ogg []\n",
      "2782_SSW_20170701.ogg []\n",
      "2782_SSW_20170701.ogg ['goowoo1']\n",
      "44957_COR_20190923.ogg ['westan']\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg ['rthhum']\n",
      "44957_COR_20190923.ogg ['rthhum']\n",
      "44957_COR_20190923.ogg ['rthhum']\n",
      "44957_COR_20190923.ogg ['rthhum']\n",
      "44957_COR_20190923.ogg ['rthhum']\n",
      "44957_COR_20190923.ogg ['rthhum']\n",
      "44957_COR_20190923.ogg ['rthhum']\n",
      "44957_COR_20190923.ogg ['rthhum']\n",
      "44957_COR_20190923.ogg ['rthhum']\n",
      "44957_COR_20190923.ogg ['rthhum']\n",
      "44957_COR_20190923.ogg ['rthhum']\n",
      "44957_COR_20190923.ogg ['rthhum']\n",
      "44957_COR_20190923.ogg ['rthhum']\n",
      "44957_COR_20190923.ogg ['rthhum']\n",
      "44957_COR_20190923.ogg ['rthhum' 'woothr']\n",
      "44957_COR_20190923.ogg ['rthhum']\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg ['rthhum']\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg ['chcant2']\n",
      "44957_COR_20190923.ogg ['hutvir']\n",
      "44957_COR_20190923.ogg ['hutvir']\n",
      "44957_COR_20190923.ogg ['chcant2']\n",
      "44957_COR_20190923.ogg ['westan']\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg ['westan']\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg ['wbwwre1']\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg ['rthhum']\n",
      "44957_COR_20190923.ogg ['rthhum']\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg ['grhcha1']\n",
      "44957_COR_20190923.ogg ['rehbar1']\n",
      "44957_COR_20190923.ogg ['whbnut']\n",
      "44957_COR_20190923.ogg ['ocbfly1']\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg ['chcant2' 'grhcha1']\n",
      "44957_COR_20190923.ogg ['chcant2' 'rthhum']\n",
      "44957_COR_20190923.ogg ['rthhum']\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg ['rthhum']\n",
      "44957_COR_20190923.ogg ['grycat']\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg ['rthhum']\n",
      "44957_COR_20190923.ogg ['rthhum']\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg ['rthhum' 'btnwar']\n",
      "44957_COR_20190923.ogg ['chcant2' 'rthhum']\n",
      "44957_COR_20190923.ogg ['rthhum']\n",
      "44957_COR_20190923.ogg ['rthhum']\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg ['westan']\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg ['rthhum']\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg ['rthhum']\n",
      "44957_COR_20190923.ogg ['buwtea' 'rthhum']\n",
      "44957_COR_20190923.ogg ['rthhum']\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg ['tromoc' 'rthhum']\n",
      "44957_COR_20190923.ogg ['ocbfly1']\n",
      "44957_COR_20190923.ogg ['rthhum']\n",
      "44957_COR_20190923.ogg ['tromoc' 'rthhum']\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg ['burwar1']\n",
      "44957_COR_20190923.ogg ['chcant2']\n",
      "44957_COR_20190923.ogg ['chcant2']\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg ['rthhum']\n",
      "44957_COR_20190923.ogg ['rthhum']\n",
      "44957_COR_20190923.ogg ['rthhum']\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg ['chcant2']\n",
      "44957_COR_20190923.ogg ['chcant2']\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg ['chcant2']\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg ['ducfly']\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg ['chcant2']\n",
      "44957_COR_20190923.ogg ['hutvir']\n",
      "44957_COR_20190923.ogg ['chbchi']\n",
      "44957_COR_20190923.ogg ['chbchi']\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg []\n",
      "44957_COR_20190923.ogg []\n",
      "14473_SSW_20170701.ogg []\n",
      "14473_SSW_20170701.ogg []\n",
      "14473_SSW_20170701.ogg []\n",
      "14473_SSW_20170701.ogg []\n",
      "14473_SSW_20170701.ogg []\n",
      "14473_SSW_20170701.ogg []\n",
      "14473_SSW_20170701.ogg []\n",
      "14473_SSW_20170701.ogg []\n",
      "14473_SSW_20170701.ogg []\n",
      "14473_SSW_20170701.ogg []\n",
      "14473_SSW_20170701.ogg []\n",
      "14473_SSW_20170701.ogg []\n",
      "14473_SSW_20170701.ogg []\n",
      "14473_SSW_20170701.ogg []\n",
      "14473_SSW_20170701.ogg []\n",
      "14473_SSW_20170701.ogg ['fiespa']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14473_SSW_20170701.ogg []\n",
      "14473_SSW_20170701.ogg ['fiespa' 'canwar']\n",
      "14473_SSW_20170701.ogg []\n",
      "14473_SSW_20170701.ogg []\n",
      "14473_SSW_20170701.ogg ['fiespa']\n",
      "14473_SSW_20170701.ogg []\n",
      "14473_SSW_20170701.ogg []\n",
      "14473_SSW_20170701.ogg []\n",
      "14473_SSW_20170701.ogg []\n",
      "14473_SSW_20170701.ogg []\n",
      "14473_SSW_20170701.ogg []\n",
      "14473_SSW_20170701.ogg []\n",
      "14473_SSW_20170701.ogg []\n",
      "14473_SSW_20170701.ogg []\n",
      "14473_SSW_20170701.ogg []\n",
      "14473_SSW_20170701.ogg []\n",
      "14473_SSW_20170701.ogg []\n",
      "14473_SSW_20170701.ogg []\n",
      "14473_SSW_20170701.ogg []\n",
      "14473_SSW_20170701.ogg ['gcrwar']\n",
      "14473_SSW_20170701.ogg ['gcrwar']\n",
      "14473_SSW_20170701.ogg []\n",
      "14473_SSW_20170701.ogg ['gcrwar']\n",
      "14473_SSW_20170701.ogg []\n",
      "14473_SSW_20170701.ogg []\n",
      "14473_SSW_20170701.ogg []\n",
      "14473_SSW_20170701.ogg []\n",
      "14473_SSW_20170701.ogg []\n",
      "14473_SSW_20170701.ogg []\n",
      "14473_SSW_20170701.ogg []\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-e87bab7106d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mduration\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0moutput_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0moutput_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clipwise_output'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    882\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-d7eb3c043471>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, mixup_lambda)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    882\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    882\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    882\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/torch/hub/zhanghang1989_ResNeSt_master/resnest/torch/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropblock_prob\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropblock1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    882\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0mbn_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0mbn_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         r\"\"\"\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_parameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m'_buffers'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0m_buffers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_buffers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_buffers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#inference\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "saved_model = './result/sed.pth'\n",
    "val_path = './asset/birdclef-2021/train_soundscapes/'\n",
    "val_list = [i for i in os.listdir(val_path) if i.split('.')[-1]=='ogg']\n",
    "val_df = pd.read_csv('./asset/birdclef-2021/train_soundscape_labels.csv')\n",
    "\n",
    "classes_list = np.array(blist)\n",
    "\n",
    "model = Tmodel(False).to(torch.float32)\n",
    "model.load_state_dict(torch.load(saved_model,map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "model.eval()\n",
    "for sound_scape in val_list:\n",
    "    sr = 32000\n",
    "    duration = 5 * sr\n",
    "    scape, _ = librosa.load(os.path.join(val_path,sound_scape),sr=sr)\n",
    "    \n",
    "    for i in range(0,len(scape),duration):\n",
    "        temp = scape[i:i+duration]\n",
    "        temp = torch.from_numpy(temp[np.newaxis,:]).to(device)\n",
    "        output_dict = model(temp)\n",
    "        \n",
    "        output_pred = output_dict['clipwise_output'].cpu().detach().numpy()\n",
    "        pred = classes_list[output_pred[0]>0.3]\n",
    "        print(sound_scape,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/zhanghang1989_ResNeSt_master\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Epoch : 1 21/3144 loss : 0.05718204006552696 / lr : 0.009755282581475812'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-1824a28bafd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m#train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mwav\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbird\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-87405e6df60b>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mbirds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmeta\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0mwav\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0mwav\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwav\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduration\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;31m# Load the target number of frames, and transpose to match librosa form\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msf_desc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mframe_duration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malways_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/soundfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, frames, dtype, always_2d, fill_value, out)\u001b[0m\n\u001b[1;32m    865\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mframes\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mframes\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m                 \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_array_io\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'read'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfill_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/soundfile.py\u001b[0m in \u001b[0;36m_array_io\u001b[0;34m(self, action, array, frames)\u001b[0m\n\u001b[1;32m   1310\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_ffi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msizeof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m         \u001b[0mcdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ffi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctype\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'*'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array_interface__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1312\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cdata_io\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_cdata_io\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/soundfile.py\u001b[0m in \u001b[0;36m_cdata_io\u001b[0;34m(self, action, data, ctype, frames)\u001b[0m\n\u001b[1;32m   1319\u001b[0m             \u001b[0mcurr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_snd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sf_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'f_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mctype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m         \u001b[0m_error_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_errorcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseekable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#train\n",
    "import torch\n",
    "from IPython.display import display\n",
    "from utils.logging import Averager\n",
    "from torch.optim import lr_scheduler\n",
    "from utils.loss import AsymmetricLoss\n",
    "\n",
    "#learning_rate = 0.1 #for onecycle\n",
    "learning_rate = 0.01  #for cosine\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "epochs=15\n",
    "model = Tmodel().to(torch.float32)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, \n",
    "        betas=(0.9, 0.999), eps=1e-08, weight_decay=0., amsgrad=True)\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "avg = Averager()\n",
    "#scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, steps_per_epoch=len(dataloader), epochs=epochs,\n",
    "#                                        pct_start=0.2)\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "dh = display('',display_id=True)\n",
    "batch_max = len(dataloader)\n",
    "loss_func = BCEFocal2WayLoss()\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    #train\n",
    "    for batch, data in enumerate(dataloader):\n",
    "        wav, bird = data\n",
    "        \n",
    "        wav = torch.from_numpy(wav).to(torch.float32)\n",
    "        wav = wav.to(device)\n",
    "        \n",
    "        bird_smooth = np.where(bird==1,0.995,0.0025)\n",
    "        bird_smooth = torch.from_numpy(bird_smooth).to(torch.float32).to(device)\n",
    "        output_dict = model(wav)\n",
    "        \n",
    "        loss = loss_func(output_dict, bird_smooth)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        scheduler.step()\n",
    "        avg.add(loss)\n",
    "        dh.update('Epoch : {} {}/{} loss : {} / lr : {}'.format(\\\n",
    "                                epoch+1,batch+1,batch_max,avg.val(),\\\n",
    "                                optimizer.param_groups[0]['lr']))\n",
    "        \n",
    "        #del wav, bird, loss, output_dict, data\n",
    "        torch.save(model.state_dict(),os.path.join('./result/sed_asloss.pth'))\n",
    "    #eval\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0047, 0.0058, 0.0096,  ..., 0.0081, 0.0099, 0.0387],\n",
       "        [0.0047, 0.0058, 0.0096,  ..., 0.0081, 0.0099, 0.0387],\n",
       "        [0.0047, 0.0058, 0.0096,  ..., 0.0081, 0.0099, 0.0387],\n",
       "        ...,\n",
       "        [0.0047, 0.0058, 0.0096,  ..., 0.0081, 0.0099, 0.0387],\n",
       "        [0.0047, 0.0058, 0.0096,  ..., 0.0081, 0.0099, 0.0387],\n",
       "        [0.0047, 0.0058, 0.0096,  ..., 0.0081, 0.0099, 0.0387]],\n",
       "       device='cuda:0', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict['clipwise_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
