{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import os\n",
    "import IPython.display as lpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import copy\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#get list of all files with labels\n",
    "data_path = './asset/birdclef-2021/train_short_audio/'\n",
    "\n",
    "df = pd.read_csv('./asset/birdclef-2021/train_metadata.csv')\n",
    "blist=df['primary_label'].unique()\n",
    "\n",
    "num_count = 1\n",
    "call_list=list()\n",
    "for sec,pri,filename in zip(df['secondary_labels'],df['primary_label'],df['filename']):\n",
    "    sec_2 = list(sec.replace(\"'\",'').replace('[','').replace(']','').split(','))\n",
    "    sec_2.append(pri)\n",
    "    if sec_2[0]=='':\n",
    "        sec_2=sec_2[1:]\n",
    "    call_list.append([os.path.join(data_path,pri,filename),sec_2])\n",
    "len(call_list)\n",
    "\n",
    "blist=list(blist)\n",
    "blist.append('nocall')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_call_window(wav,duration=32000*5,mode='precise'):\n",
    "    #Todo : implementing get multiple windows option would be needed\n",
    "    \n",
    "    if not mode in ['precise','fast']:\n",
    "        raise ValueError('get_call_window mode parameter allow \"precise\" and \"fast\"')\n",
    "    \n",
    "    mean = sum(np.abs(wav))/len(wav)\n",
    "    call = None\n",
    "    silence = None\n",
    "    for i in range(0,len(wav),32000):\n",
    "        if i+duration > len(wav):\n",
    "            break\n",
    "        if sum(np.abs(wav[i:i+duration]))/duration > mean:\n",
    "            if call is None: call = wav[i:i+duration]\n",
    "            else:\n",
    "                if sum(np.abs(call))/duration < sum(np.abs(wav[i:i+duration]))/duration:\n",
    "                    call = wav[i:i+duration]\n",
    "        else:\n",
    "            if silence is None: silence = wav[i:i+duration]\n",
    "            else:\n",
    "                if sum(np.abs(silence))/duration > sum(np.abs(wav[i:i+duration]))/duration:\n",
    "                    silence = wav[i:i+duration]\n",
    "        if mode == 'fast' and (call is not None and silence is not None):\n",
    "            return call, silence\n",
    "    return call,silence\n",
    "\n",
    "def convert_to_mel(wav):\n",
    "    SPEC_HEIGHT = 64\n",
    "    SPEC_WIDTH = 256\n",
    "    NUM_MELS = SPEC_HEIGHT\n",
    "    HOP_LENGTH = int(32000 * 5 / (SPEC_WIDTH - 1)) # sample rate * duration / spec width - 1 == 627\n",
    "    FMIN = 500\n",
    "    FMAX = 12500\n",
    "    mel_spec = librosa.feature.melspectrogram(y=wav, \n",
    "                                              sr=32000, \n",
    "                                              n_fft=1024, \n",
    "                                              hop_length=HOP_LENGTH, \n",
    "                                              n_mels=NUM_MELS, \n",
    "                                              fmin=FMIN, \n",
    "                                              fmax=FMAX)\n",
    "    mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    mel_spec -= mel_spec.min()\n",
    "    mel_spec /= mel_spec.max()\n",
    "    return mel_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import copy\n",
    "import random\n",
    "\n",
    "pickle_list = copy.deepcopy(call_list)\n",
    "random.shuffle(pickle_list)\n",
    "\n",
    "num_data = 400\n",
    "\n",
    "nocall_bag = list()\n",
    "nocall_max = num_data//10\n",
    "new_list = list()\n",
    "for i,data in enumerate(pickle_list):\n",
    "    print(i)\n",
    "    if i > num_data:\n",
    "        break\n",
    "    wav, _ =librosa.load(data[0],sr=32000)\n",
    "    wav = librosa.util.normalize(wav)\n",
    "    call, silence = get_call_window(wav,mode='fast')\n",
    "    new_list.append([call,data[1]])\n",
    "    if nocall_max>len(nocall_bag):\n",
    "        nocall_bag.append([silence,['nocall']])\n",
    "\n",
    "new_list = new_list+nocall_bag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "mel_list = list()\n",
    "for i,data in enumerate(new_list):\n",
    "    if i%100==0:\n",
    "        print(i)\n",
    "    BCencoding = np.zeros((len(blist)))\n",
    "    for bird in data[1]:\n",
    "        if bird in blist:\n",
    "            BCencoding[blist.index(bird)] = 1\n",
    "    if data[0] is None:\n",
    "        continue\n",
    "    mel = convert_to_mel(data[0])\n",
    "    mel_list.append([mel,BCencoding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './asset/temp.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d9dee74bd0e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m ''' \n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./asset/temp.pkl'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mmel_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './asset/temp.pkl'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "'''\n",
    "with open('./asset/temp.pkl','wb') as f:\n",
    "    pickle.dump(mel_list,f)\n",
    "''' \n",
    "with open('./asset/temp.pkl','rb') as f:\n",
    "    mel_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from modules.ResNet import resnet152 , resnet50\n",
    "import torch.optim as optim\n",
    "from IPython.display import display\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from utils.loss import AsymmetricLoss\n",
    "from utils.ema import ModelEma\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#model = resnet152(num_classes = len(blist))\n",
    "model = resnet152(num_classes = len(blist))\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "class Averager(object):\n",
    "    \"\"\"Compute average for torch.Tensor, used for loss average.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def add(self, v):\n",
    "        count = v.data.numel()\n",
    "        v = v.data.sum()\n",
    "        self.n_count += count\n",
    "        self.sum += v\n",
    "\n",
    "    def reset(self):\n",
    "        self.n_count = 0\n",
    "        self.sum = 0\n",
    "\n",
    "    def val(self):\n",
    "        res = 0\n",
    "        if self.n_count != 0:\n",
    "            res = self.sum / float(self.n_count)\n",
    "        return res\n",
    "    \n",
    "def get_multi_batch(dataset,cur,batch_size):\n",
    "    call = np.concatenate([data[0][np.newaxis,np.newaxis,:,:] for data in dataset[cur:cur+batch_size]])\n",
    "    lbl = np.concatenate([np.reshape(data[1],(1,len(blist))) for data in dataset[cur:cur+batch_size]])\n",
    "    call = torch.from_numpy(call).to(device)\n",
    "    lbl = torch.from_numpy(lbl).to(device)\n",
    "    \n",
    "    return call,lbl\n",
    "\n",
    "dataset = mel_list\n",
    "\n",
    "ema = ModelEma(model, 0.9997)\n",
    "loss_avg = Averager()\n",
    "criterion = AsymmetricLoss(gamma_neg=4, gamma_pos=0, clip=0.05, disable_torch_grad_focal_loss=True)\n",
    "\n",
    "filtered_parameters = []\n",
    "params_num = []\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "for p in filter(lambda p: p.requires_grad, model.parameters()):\n",
    "    filtered_parameters.append(p)\n",
    "    params_num.append(np.prod(p.size()))\n",
    "print('Trainable params num : ', sum(params_num))\n",
    "\n",
    "learning_rate=0.0003\n",
    "optimizer = optim.Adam(filtered_parameters, lr=learning_rate, betas=(0.9, 0.999))\n",
    "batch_size = 10\n",
    "epochs = 100\n",
    "df = display('go',display_id=True)\n",
    "scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, steps_per_epoch=len(dataset), epochs=epochs,\n",
    "                                        pct_start=0.2)\n",
    "for epoch in range(epochs):\n",
    "    for batch in range(len(dataset)//batch_size):\n",
    "        \n",
    "        call, lbl = get_multi_batch(dataset,batch,batch_size)\n",
    "        #test_data = np.concatenate([data[0][np.newaxis,np.newaxis,:,:] for data in dataset[:batch_size]])\n",
    "        with autocast():  # mixed precision\n",
    "            output = model(call).float()  # sigmoid will be done in loss !\n",
    "        \n",
    "        loss = criterion(output, lbl)\n",
    "        loss_avg.add(loss)\n",
    "        model.zero_grad()\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        # loss.backward()\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        # optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        ema.update(model)\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),5)  # gradient clipping with 5 (Default)\n",
    "        #https://kh-kim.gitbook.io/natural-language-processing-with-pytorch/00-cover-6/05-gradient-clipping\n",
    "        df.update(f'epoch : {epoch} / batch : {batch} / loss : {loss_avg.val().item()}')\n",
    "        \n",
    "        del call, lbl, output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind2bird = dict()\n",
    "for i,bird in enumerate(blist):\n",
    "    ind2bird[i] = bird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scape_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import librosa\n",
    "import numpy as np\n",
    "from modules.ResNet import resnet152, resnet50\n",
    "import torch.optim as optim\n",
    "from IPython.display import display\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from utils.loss import AsymmetricLoss\n",
    "from utils.ema import ModelEma\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from data.preprocess import get_feature_extractor,mono_to_color\n",
    "import pickle\n",
    "\n",
    "with open('./train_data/temp_2-003.pkl','rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "scape_df = pd.read_csv('./asset/birdclef-2021/train_soundscape_labels.csv')\n",
    "scape_path = './asset/birdclef-2021/train_soundscapes/'\n",
    "scape_list = os.listdir(scape_path)\n",
    "f1_score_sum = 0\n",
    "f1_score_num = 0\n",
    "\n",
    "\n",
    "model = resnet152(num_classes = 397)\n",
    "#model = resnet50(num_classes = 397)\n",
    "feature_extractor = get_feature_extractor()\n",
    "\n",
    "\n",
    "saved_model = './result/model_1620811543.8092575.pth'\n",
    "#saved_model = './result/lr_0.003_res50.pth'\n",
    "model.load_state_dict(torch.load(saved_model, map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "model.eval()\n",
    "classes_list = np.array(blist)[:-1]\n",
    "\n",
    "scape_dic=dict()\n",
    "for row,sec,birds in zip(scape_df['row_id'],scape_df['seconds'],scape_df['birds']):\n",
    "    temp = row.split('_')[:-1]\n",
    "    filename = '_'.join(temp)\n",
    "    if filename in scape_dic.keys():\n",
    "        scape_dic[filename].append([sec,birds])\n",
    "    else:\n",
    "        scape_dic[filename]=[[sec,birds]]\n",
    "\n",
    "        \n",
    "\n",
    "for scape in scape_list:\n",
    "    if scape.split('.')[-1]!='ogg':\n",
    "        continue\n",
    "        \n",
    "    wav, _ =librosa.load(os.path.join(scape_path,scape),sr=32000)\n",
    "    wav = librosa.util.normalize(wav)\n",
    "    \n",
    "    temp = scape.split('_')[:-1]\n",
    "    filename = '_'.join(temp)\n",
    "    for frame in scape_dic[filename]:\n",
    "        point = frame[0]*32000\n",
    "        clip = wav[point-32000*5:point]\n",
    "        #lpd.display(lpd.Audio(clip,rate=32000))\n",
    "        clip = torch.from_numpy(clip[np.newaxis,:]).to(device)\n",
    "        \n",
    "        mel_spec = feature_extractor(clip)\n",
    "        mel_spec = torch.from_numpy(mono_to_color(mel_spec.cpu().detach().numpy())).to(device)\n",
    "\n",
    "        #print(mel_spec)\n",
    "        output = torch.squeeze(F.softmax(model(mel_spec.to(torch.float32))))\n",
    "        np_output = output.cpu().detach().numpy()\n",
    "        print(classes_list[np_output>0.3],frame[1].split(' '))\n",
    "'''\n",
    "\n",
    "for row in data:\n",
    "    mel_spec = torch.from_numpy(row[0][np.newaxis,np.newaxis,:,:]).to(device)\n",
    "    output = torch.squeeze(F.softmax(model(mel_spec.to(torch.float32))))\n",
    "    np_output = output.cpu().detach().numpy()\n",
    "    print(classes_list[np_output>0.3],classes_list[row[1]>0.5])\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import librosa\n",
    "import numpy as np\n",
    "from modules.ResNet import resnet152, resnet50\n",
    "import torch.optim as optim\n",
    "from IPython.display import display\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from utils.loss import AsymmetricLoss\n",
    "from utils.ema import ModelEma\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from data.preprocess import get_feature_extractor,mono_to_color\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#model = resnet152(num_classes = 397)\n",
    "model = resnet50(num_classes = 397)\n",
    "feature_extractor = get_feature_extractor()\n",
    "\n",
    "\n",
    "\n",
    "#saved_model = './result/1620829476.648939.pth'\n",
    "saved_model = './result/lr_0.003_res50.pth'\n",
    "model.load_state_dict(torch.load(saved_model, map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "data_path = './asset/birdclef-2021/train_soundscapes/10534_SSW_20170429.ogg'\n",
    "wav, _ =librosa.load(data_path,sr=32000)\n",
    "wav = librosa.util.normalize(wav)\n",
    "test_list=list()\n",
    "model.eval()\n",
    "classes_list = np.array(blist)[:-1]\n",
    "for i in range(0,len(wav),32000*5):\n",
    "    temp = wav[i:i+32000*5]\n",
    "    temp = torch.from_numpy(temp[np.newaxis,:]).to(device)\n",
    "    mel_spec = feature_extractor(temp)\n",
    "    mel_spec = torch.from_numpy(mono_to_color(mel_spec.cpu().detach().numpy())).to(device)\n",
    "    \n",
    "    #print(mel_spec)\n",
    "    output = torch.squeeze(F.softmax(model(mel_spec.to(torch.float32))))\n",
    "    np_output = output.cpu().detach().numpy()\n",
    "    print(classes_list[np_output>0.3])\n",
    "    #detected_classes = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dataset:\n",
    "    print(classes_list[i[1]>=1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "a = [0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "b = [0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "\n",
    "print(len(a))\n",
    "print(f1_score(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
